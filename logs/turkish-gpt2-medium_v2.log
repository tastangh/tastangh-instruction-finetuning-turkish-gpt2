2024-12-24 23:07:12,531 - INFO - [INFO] Eğitim başlıyor: Model=ytu-ce-cosmos/turkish-gpt2-medium, Dataset=v2
2024-12-24 23:07:16,672 - INFO - Dataset başarıyla yüklendi: v2
2024-12-24 23:07:16,673 - INFO - Model ve tokenizer yükleniyor: ytu-ce-cosmos/turkish-gpt2-medium
2024-12-24 23:07:18,532 - INFO - Model ve tokenizer başarıyla yüklendi: ytu-ce-cosmos/turkish-gpt2-medium
2024-12-24 23:07:18,532 - INFO - Model ytu-ce-cosmos/turkish-gpt2-medium ince ayar işlemi başlatılıyor.
2024-12-24 23:07:37,096 - INFO - Dataset başarıyla işlendi: max_seq_length=256
2024-12-24 23:09:00,985 - INFO - Training Logs at step 100: {'loss': 27.9782, 'grad_norm': 6.487659931182861, 'learning_rate': 2.35e-05, 'epoch': 0.007757500533328162}
2024-12-24 23:09:44,786 - INFO - Training Logs at step 150: {'loss': 26.8132, 'grad_norm': 16.398271560668945, 'learning_rate': 4.85e-05, 'epoch': 0.015515001066656323}
2024-12-24 23:10:29,986 - INFO - Training Logs at step 200: {'loss': 25.1021, 'grad_norm': 11.761281967163086, 'learning_rate': 4.962962962962963e-05, 'epoch': 0.023272501599984487}
2024-12-24 23:11:14,693 - INFO - Training Logs at step 250: {'loss': 24.4795, 'grad_norm': 11.123762130737305, 'learning_rate': 4.9235618597320725e-05, 'epoch': 0.031030002133312647}
2024-12-24 23:11:59,440 - INFO - Training Logs at step 300: {'loss': 24.0786, 'grad_norm': 15.25679874420166, 'learning_rate': 4.884160756501182e-05, 'epoch': 0.03878750266664081}
2024-12-24 23:12:44,965 - INFO - Training Logs at step 350: {'loss': 23.9169, 'grad_norm': 14.17778205871582, 'learning_rate': 4.844759653270291e-05, 'epoch': 0.04654500319996897}
2024-12-24 23:13:29,742 - INFO - Training Logs at step 400: {'loss': 23.5236, 'grad_norm': 11.824249267578125, 'learning_rate': 4.805358550039401e-05, 'epoch': 0.05430250373329713}
2024-12-24 23:14:14,722 - INFO - Training Logs at step 450: {'loss': 23.7322, 'grad_norm': 11.57138442993164, 'learning_rate': 4.7659574468085114e-05, 'epoch': 0.06206000426662529}
2024-12-24 23:15:00,044 - INFO - Training Logs at step 500: {'loss': 23.5381, 'grad_norm': 20.203702926635742, 'learning_rate': 4.726556343577621e-05, 'epoch': 0.06981750479995345}
2024-12-24 23:15:44,646 - INFO - Training Logs at step 550: {'loss': 23.3721, 'grad_norm': 14.90483570098877, 'learning_rate': 4.68715524034673e-05, 'epoch': 0.07757500533328161}
2024-12-24 23:16:29,352 - INFO - Training Logs at step 600: {'loss': 23.5244, 'grad_norm': 18.52785301208496, 'learning_rate': 4.6477541371158395e-05, 'epoch': 0.08533250586660977}
2024-12-24 23:17:14,824 - INFO - Training Logs at step 650: {'loss': 23.3923, 'grad_norm': 21.06534194946289, 'learning_rate': 4.608353033884949e-05, 'epoch': 0.09309000639993795}
2024-12-24 23:17:59,438 - INFO - Training Logs at step 700: {'loss': 23.1635, 'grad_norm': 17.559200286865234, 'learning_rate': 4.568951930654058e-05, 'epoch': 0.1008475069332661}
2024-12-24 23:18:44,413 - INFO - Training Logs at step 750: {'loss': 23.345, 'grad_norm': 13.68657398223877, 'learning_rate': 4.529550827423168e-05, 'epoch': 0.10860500746659427}
2024-12-24 23:19:29,757 - INFO - Training Logs at step 800: {'loss': 23.0449, 'grad_norm': 17.277957916259766, 'learning_rate': 4.4901497241922776e-05, 'epoch': 0.11636250799992243}
2024-12-24 23:20:14,567 - INFO - Training Logs at step 850: {'loss': 23.4708, 'grad_norm': 11.27308464050293, 'learning_rate': 4.450748620961387e-05, 'epoch': 0.12412000853325059}
2024-12-24 23:20:59,828 - INFO - Training Logs at step 900: {'loss': 22.8121, 'grad_norm': 16.334070205688477, 'learning_rate': 4.411347517730497e-05, 'epoch': 0.13187750906657875}
2024-12-24 23:21:45,060 - INFO - Training Logs at step 950: {'loss': 23.5065, 'grad_norm': 17.981678009033203, 'learning_rate': 4.3719464144996064e-05, 'epoch': 0.1396350095999069}
2024-12-24 23:22:29,435 - INFO - Training Logs at step 1000: {'loss': 23.331, 'grad_norm': 13.48510456085205, 'learning_rate': 4.332545311268716e-05, 'epoch': 0.14739251013323507}
2024-12-24 23:23:14,444 - INFO - Training Logs at step 1050: {'loss': 23.2015, 'grad_norm': 19.300247192382812, 'learning_rate': 4.293144208037825e-05, 'epoch': 0.15515001066656323}
2024-12-24 23:23:59,232 - INFO - Training Logs at step 1100: {'loss': 23.4588, 'grad_norm': 13.32846736907959, 'learning_rate': 4.2537431048069345e-05, 'epoch': 0.16290751119989139}
2024-12-24 23:24:43,884 - INFO - Training Logs at step 1150: {'loss': 23.3336, 'grad_norm': 12.858319282531738, 'learning_rate': 4.2143420015760446e-05, 'epoch': 0.17066501173321955}
2024-12-24 23:25:28,778 - INFO - Training Logs at step 1200: {'loss': 23.4534, 'grad_norm': 14.906665802001953, 'learning_rate': 4.174940898345154e-05, 'epoch': 0.1784225122665477}
2024-12-24 23:26:13,146 - INFO - Training Logs at step 1250: {'loss': 22.8117, 'grad_norm': 13.576480865478516, 'learning_rate': 4.135539795114263e-05, 'epoch': 0.1861800127998759}
2024-12-24 23:26:54,629 - INFO - Training Logs at step 1300: {'loss': 22.8505, 'grad_norm': 14.536890029907227, 'learning_rate': 4.096138691883373e-05, 'epoch': 0.19393751333320405}
2024-12-24 23:27:36,027 - INFO - Training Logs at step 1350: {'loss': 23.3807, 'grad_norm': 14.580432891845703, 'learning_rate': 4.056737588652482e-05, 'epoch': 0.2016950138665322}
2024-12-24 23:28:17,501 - INFO - Training Logs at step 1400: {'loss': 22.8608, 'grad_norm': 11.0767240524292, 'learning_rate': 4.017336485421592e-05, 'epoch': 0.20945251439986037}
2024-12-24 23:28:59,011 - INFO - Training Logs at step 1450: {'loss': 22.8555, 'grad_norm': 20.33308982849121, 'learning_rate': 3.9779353821907015e-05, 'epoch': 0.21721001493318853}
2024-12-24 23:29:40,560 - INFO - Training Logs at step 1500: {'loss': 23.4951, 'grad_norm': 12.997675895690918, 'learning_rate': 3.9385342789598115e-05, 'epoch': 0.2249675154665167}
2024-12-24 23:30:22,360 - INFO - Training Logs at step 1550: {'loss': 23.2416, 'grad_norm': 11.190990447998047, 'learning_rate': 3.899133175728921e-05, 'epoch': 0.23272501599984485}
2024-12-24 23:31:04,445 - INFO - Training Logs at step 1600: {'loss': 23.0301, 'grad_norm': 10.820331573486328, 'learning_rate': 3.85973207249803e-05, 'epoch': 0.240482516533173}
2024-12-24 23:31:46,221 - INFO - Training Logs at step 1650: {'loss': 23.0499, 'grad_norm': 16.60434341430664, 'learning_rate': 3.8203309692671396e-05, 'epoch': 0.24824001706650117}
2024-12-24 23:32:27,501 - INFO - Training Logs at step 1700: {'loss': 22.7466, 'grad_norm': 16.191808700561523, 'learning_rate': 3.780929866036249e-05, 'epoch': 0.25599751759982936}
2024-12-24 23:33:08,896 - INFO - Training Logs at step 1750: {'loss': 22.834, 'grad_norm': 15.938565254211426, 'learning_rate': 3.7415287628053584e-05, 'epoch': 0.2637550181331575}
2024-12-24 23:33:50,102 - INFO - Training Logs at step 1800: {'loss': 22.7703, 'grad_norm': 15.760249137878418, 'learning_rate': 3.702127659574468e-05, 'epoch': 0.2715125186664857}
2024-12-24 23:34:31,511 - INFO - Training Logs at step 1850: {'loss': 23.0462, 'grad_norm': 17.323955535888672, 'learning_rate': 3.662726556343578e-05, 'epoch': 0.2792700191998138}
2024-12-24 23:35:13,159 - INFO - Training Logs at step 1900: {'loss': 22.7141, 'grad_norm': 11.889152526855469, 'learning_rate': 3.623325453112688e-05, 'epoch': 0.287027519733142}
2024-12-24 23:35:54,612 - INFO - Training Logs at step 1950: {'loss': 22.921, 'grad_norm': 15.132623672485352, 'learning_rate': 3.583924349881797e-05, 'epoch': 0.29478502026647013}
2024-12-24 23:36:36,170 - INFO - Training Logs at step 2000: {'loss': 23.1512, 'grad_norm': 15.599650382995605, 'learning_rate': 3.5445232466509066e-05, 'epoch': 0.3025425207997983}
2024-12-24 23:37:17,948 - INFO - Training Logs at step 2050: {'loss': 23.1004, 'grad_norm': 15.28879451751709, 'learning_rate': 3.505122143420016e-05, 'epoch': 0.31030002133312645}
2024-12-24 23:37:59,784 - INFO - Training Logs at step 2100: {'loss': 22.9025, 'grad_norm': 19.0328426361084, 'learning_rate': 3.465721040189125e-05, 'epoch': 0.31805752186645464}
2024-12-24 23:38:40,925 - INFO - Training Logs at step 2150: {'loss': 22.8091, 'grad_norm': 14.535594940185547, 'learning_rate': 3.426319936958235e-05, 'epoch': 0.32581502239978277}
2024-12-24 23:39:22,168 - INFO - Training Logs at step 2200: {'loss': 22.4378, 'grad_norm': 15.446867942810059, 'learning_rate': 3.386918833727345e-05, 'epoch': 0.33357252293311096}
2024-12-24 23:40:03,943 - INFO - Training Logs at step 2250: {'loss': 22.6342, 'grad_norm': 18.030778884887695, 'learning_rate': 3.347517730496454e-05, 'epoch': 0.3413300234664391}
2024-12-24 23:40:45,222 - INFO - Training Logs at step 2300: {'loss': 22.7823, 'grad_norm': 21.827651977539062, 'learning_rate': 3.3081166272655635e-05, 'epoch': 0.3490875239997673}
2024-12-24 23:41:26,457 - INFO - Training Logs at step 2350: {'loss': 22.4072, 'grad_norm': 15.612634658813477, 'learning_rate': 3.2687155240346735e-05, 'epoch': 0.3568450245330954}
2024-12-24 23:42:07,939 - INFO - Training Logs at step 2400: {'loss': 22.822, 'grad_norm': 15.150609970092773, 'learning_rate': 3.229314420803783e-05, 'epoch': 0.3646025250664236}
2024-12-24 23:42:49,464 - INFO - Training Logs at step 2450: {'loss': 22.7988, 'grad_norm': 14.256942749023438, 'learning_rate': 3.19070133963751e-05, 'epoch': 0.3723600255997518}
2024-12-24 23:43:31,294 - INFO - Training Logs at step 2500: {'loss': 22.5555, 'grad_norm': 12.470438957214355, 'learning_rate': 3.15130023640662e-05, 'epoch': 0.3801175261330799}
2024-12-24 23:44:12,704 - INFO - Training Logs at step 2550: {'loss': 22.4446, 'grad_norm': 11.426474571228027, 'learning_rate': 3.111899133175729e-05, 'epoch': 0.3878750266664081}
2024-12-24 23:44:53,956 - INFO - Training Logs at step 2600: {'loss': 22.6501, 'grad_norm': 14.503941535949707, 'learning_rate': 3.0724980299448387e-05, 'epoch': 0.39563252719973624}
2024-12-24 23:45:35,373 - INFO - Training Logs at step 2650: {'loss': 22.6225, 'grad_norm': 17.176668167114258, 'learning_rate': 3.033096926713948e-05, 'epoch': 0.4033900277330644}
2024-12-24 23:46:16,585 - INFO - Training Logs at step 2700: {'loss': 22.6197, 'grad_norm': 12.689443588256836, 'learning_rate': 2.9936958234830574e-05, 'epoch': 0.41114752826639256}
2024-12-24 23:46:57,725 - INFO - Training Logs at step 2750: {'loss': 23.0361, 'grad_norm': 17.353927612304688, 'learning_rate': 2.954294720252167e-05, 'epoch': 0.41890502879972075}
2024-12-24 23:47:38,591 - INFO - Training Logs at step 2800: {'loss': 22.7008, 'grad_norm': 15.270110130310059, 'learning_rate': 2.9148936170212765e-05, 'epoch': 0.4266625293330489}
2024-12-24 23:48:19,346 - INFO - Training Logs at step 2850: {'loss': 22.9795, 'grad_norm': 15.123201370239258, 'learning_rate': 2.8754925137903865e-05, 'epoch': 0.43442002986637707}
2024-12-24 23:49:00,133 - INFO - Training Logs at step 2900: {'loss': 22.8744, 'grad_norm': 16.14456558227539, 'learning_rate': 2.836091410559496e-05, 'epoch': 0.4421775303997052}
2024-12-24 23:49:41,420 - INFO - Training Logs at step 2950: {'loss': 22.2385, 'grad_norm': 13.955867767333984, 'learning_rate': 2.7966903073286056e-05, 'epoch': 0.4499350309330334}
2024-12-24 23:50:23,549 - INFO - Training Logs at step 3000: {'loss': 22.9939, 'grad_norm': 15.486296653747559, 'learning_rate': 2.757289204097715e-05, 'epoch': 0.4576925314663615}
2024-12-24 23:51:05,355 - INFO - Training Logs at step 3050: {'loss': 22.9194, 'grad_norm': 13.975177764892578, 'learning_rate': 2.7178881008668243e-05, 'epoch': 0.4654500319996897}
2024-12-24 23:51:47,144 - INFO - Training Logs at step 3100: {'loss': 22.5945, 'grad_norm': 15.450186729431152, 'learning_rate': 2.678486997635934e-05, 'epoch': 0.47320753253301784}
2024-12-24 23:52:28,770 - INFO - Training Logs at step 3150: {'loss': 21.9307, 'grad_norm': 13.239785194396973, 'learning_rate': 2.6390858944050434e-05, 'epoch': 0.480965033066346}
2024-12-24 23:53:10,452 - INFO - Training Logs at step 3200: {'loss': 22.3146, 'grad_norm': 18.87943458557129, 'learning_rate': 2.5996847911741528e-05, 'epoch': 0.48872253359967416}
2024-12-24 23:53:51,913 - INFO - Training Logs at step 3250: {'loss': 22.5597, 'grad_norm': 13.241585731506348, 'learning_rate': 2.5602836879432625e-05, 'epoch': 0.49648003413300235}
2024-12-24 23:54:33,674 - INFO - Training Logs at step 3300: {'loss': 22.3326, 'grad_norm': 20.241405487060547, 'learning_rate': 2.520882584712372e-05, 'epoch': 0.5042375346663305}
2024-12-24 23:55:15,456 - INFO - Training Logs at step 3350: {'loss': 22.9866, 'grad_norm': 19.346397399902344, 'learning_rate': 2.4814814814814816e-05, 'epoch': 0.5119950351996587}
2024-12-24 23:55:57,800 - INFO - Training Logs at step 3400: {'loss': 22.3747, 'grad_norm': 15.2630615234375, 'learning_rate': 2.442080378250591e-05, 'epoch': 0.5197525357329869}
2024-12-24 23:56:39,398 - INFO - Training Logs at step 3450: {'loss': 22.9586, 'grad_norm': 15.454251289367676, 'learning_rate': 2.4026792750197007e-05, 'epoch': 0.527510036266315}
2024-12-24 23:57:20,883 - INFO - Training Logs at step 3500: {'loss': 22.5128, 'grad_norm': 10.664538383483887, 'learning_rate': 2.3632781717888104e-05, 'epoch': 0.5352675367996431}
2024-12-24 23:58:02,326 - INFO - Training Logs at step 3550: {'loss': 22.2081, 'grad_norm': 14.900432586669922, 'learning_rate': 2.3238770685579197e-05, 'epoch': 0.5430250373329714}
2024-12-24 23:58:43,763 - INFO - Training Logs at step 3600: {'loss': 22.2181, 'grad_norm': 14.587544441223145, 'learning_rate': 2.284475965327029e-05, 'epoch': 0.5507825378662995}
2024-12-24 23:59:25,133 - INFO - Training Logs at step 3650: {'loss': 22.2454, 'grad_norm': 20.33481216430664, 'learning_rate': 2.2450748620961388e-05, 'epoch': 0.5585400383996276}
2024-12-25 00:00:06,868 - INFO - Training Logs at step 3700: {'loss': 22.5937, 'grad_norm': 18.664684295654297, 'learning_rate': 2.2056737588652485e-05, 'epoch': 0.5662975389329558}
2024-12-25 00:00:48,722 - INFO - Training Logs at step 3750: {'loss': 22.3489, 'grad_norm': 14.1321439743042, 'learning_rate': 2.166272655634358e-05, 'epoch': 0.574055039466284}
2024-12-25 00:01:30,748 - INFO - Training Logs at step 3800: {'loss': 22.4277, 'grad_norm': 14.205493927001953, 'learning_rate': 2.1268715524034673e-05, 'epoch': 0.5818125399996121}
2024-12-25 00:02:12,584 - INFO - Training Logs at step 3850: {'loss': 22.45, 'grad_norm': 13.283905982971191, 'learning_rate': 2.087470449172577e-05, 'epoch': 0.5895700405329403}
2024-12-25 00:02:53,885 - INFO - Training Logs at step 3900: {'loss': 22.6396, 'grad_norm': 16.258808135986328, 'learning_rate': 2.0480693459416863e-05, 'epoch': 0.5973275410662684}
2024-12-25 00:03:35,166 - INFO - Training Logs at step 3950: {'loss': 22.3783, 'grad_norm': 19.396337509155273, 'learning_rate': 2.008668242710796e-05, 'epoch': 0.6050850415995966}
2024-12-25 00:04:16,635 - INFO - Training Logs at step 4000: {'loss': 23.0227, 'grad_norm': 16.391447067260742, 'learning_rate': 1.9692671394799058e-05, 'epoch': 0.6128425421329248}
2024-12-25 00:04:57,917 - INFO - Training Logs at step 4050: {'loss': 22.3646, 'grad_norm': 14.972280502319336, 'learning_rate': 1.929866036249015e-05, 'epoch': 0.6206000426662529}
2024-12-25 00:05:39,190 - INFO - Training Logs at step 4100: {'loss': 22.4345, 'grad_norm': 22.093286514282227, 'learning_rate': 1.8904649330181245e-05, 'epoch': 0.6283575431995811}
2024-12-25 00:06:20,355 - INFO - Training Logs at step 4150: {'loss': 22.5409, 'grad_norm': 15.444648742675781, 'learning_rate': 1.851063829787234e-05, 'epoch': 0.6361150437329093}
2024-12-25 00:07:01,435 - INFO - Training Logs at step 4200: {'loss': 22.4226, 'grad_norm': 13.998590469360352, 'learning_rate': 1.811662726556344e-05, 'epoch': 0.6438725442662374}
2024-12-25 00:07:42,814 - INFO - Training Logs at step 4250: {'loss': 22.28, 'grad_norm': 15.725584983825684, 'learning_rate': 1.7722616233254533e-05, 'epoch': 0.6516300447995655}
2024-12-25 00:08:24,442 - INFO - Training Logs at step 4300: {'loss': 22.5494, 'grad_norm': 8.894916534423828, 'learning_rate': 1.7328605200945627e-05, 'epoch': 0.6593875453328938}
2024-12-25 00:09:06,047 - INFO - Training Logs at step 4350: {'loss': 22.4966, 'grad_norm': 16.291561126708984, 'learning_rate': 1.6934594168636724e-05, 'epoch': 0.6671450458662219}
2024-12-25 00:09:47,241 - INFO - Training Logs at step 4400: {'loss': 22.2213, 'grad_norm': 13.274648666381836, 'learning_rate': 1.6540583136327817e-05, 'epoch': 0.67490254639955}
2024-12-25 00:10:28,306 - INFO - Training Logs at step 4450: {'loss': 22.1994, 'grad_norm': 17.398181915283203, 'learning_rate': 1.6146572104018914e-05, 'epoch': 0.6826600469328782}
2024-12-25 00:11:09,581 - INFO - Training Logs at step 4500: {'loss': 22.4973, 'grad_norm': 9.205543518066406, 'learning_rate': 1.5752561071710008e-05, 'epoch': 0.6904175474662064}
2024-12-25 00:11:50,679 - INFO - Training Logs at step 4550: {'loss': 22.2298, 'grad_norm': 16.514307022094727, 'learning_rate': 1.5358550039401105e-05, 'epoch': 0.6981750479995346}
2024-12-25 00:12:31,845 - INFO - Training Logs at step 4600: {'loss': 22.7941, 'grad_norm': 14.73262882232666, 'learning_rate': 1.4964539007092199e-05, 'epoch': 0.7059325485328627}
2024-12-25 00:13:13,067 - INFO - Training Logs at step 4650: {'loss': 22.7681, 'grad_norm': 11.746129035949707, 'learning_rate': 1.4578408195429472e-05, 'epoch': 0.7136900490661908}
2024-12-25 00:13:54,143 - INFO - Training Logs at step 4700: {'loss': 22.7901, 'grad_norm': 14.054067611694336, 'learning_rate': 1.418439716312057e-05, 'epoch': 0.7214475495995191}
2024-12-25 00:14:35,260 - INFO - Training Logs at step 4750: {'loss': 22.3687, 'grad_norm': 16.71718406677246, 'learning_rate': 1.3790386130811665e-05, 'epoch': 0.7292050501328472}
2024-12-25 00:15:16,328 - INFO - Training Logs at step 4800: {'loss': 22.3089, 'grad_norm': 19.803455352783203, 'learning_rate': 1.3396375098502758e-05, 'epoch': 0.7369625506661753}
2024-12-25 00:15:57,298 - INFO - Training Logs at step 4850: {'loss': 22.3104, 'grad_norm': 15.842299461364746, 'learning_rate': 1.3002364066193854e-05, 'epoch': 0.7447200511995036}
2024-12-25 00:16:38,867 - INFO - Training Logs at step 4900: {'loss': 22.5237, 'grad_norm': 12.275066375732422, 'learning_rate': 1.2608353033884947e-05, 'epoch': 0.7524775517328317}
2024-12-25 00:17:20,507 - INFO - Training Logs at step 4950: {'loss': 22.743, 'grad_norm': 13.650367736816406, 'learning_rate': 1.2214342001576045e-05, 'epoch': 0.7602350522661598}
2024-12-25 00:18:01,947 - INFO - Training Logs at step 5000: {'loss': 22.7682, 'grad_norm': 14.512455940246582, 'learning_rate': 1.182033096926714e-05, 'epoch': 0.767992552799488}
2024-12-25 00:18:42,938 - INFO - Training Logs at step 5050: {'loss': 22.0569, 'grad_norm': 19.04360008239746, 'learning_rate': 1.1426319936958235e-05, 'epoch': 0.7757500533328162}
2024-12-25 00:19:24,154 - INFO - Training Logs at step 5100: {'loss': 22.2408, 'grad_norm': 12.041986465454102, 'learning_rate': 1.103230890464933e-05, 'epoch': 0.7835075538661443}
2024-12-25 00:20:05,350 - INFO - Training Logs at step 5150: {'loss': 22.3211, 'grad_norm': 12.798806190490723, 'learning_rate': 1.0638297872340426e-05, 'epoch': 0.7912650543994725}
2024-12-25 00:20:46,464 - INFO - Training Logs at step 5200: {'loss': 22.7079, 'grad_norm': 14.44979190826416, 'learning_rate': 1.0244286840031522e-05, 'epoch': 0.7990225549328006}
2024-12-25 00:21:27,997 - INFO - Training Logs at step 5250: {'loss': 22.2478, 'grad_norm': 12.757763862609863, 'learning_rate': 9.850275807722617e-06, 'epoch': 0.8067800554661289}
2024-12-25 00:22:10,381 - INFO - Training Logs at step 5300: {'loss': 22.1725, 'grad_norm': 15.389243125915527, 'learning_rate': 9.456264775413712e-06, 'epoch': 0.814537555999457}
2024-12-25 00:22:52,569 - INFO - Training Logs at step 5350: {'loss': 21.9526, 'grad_norm': 19.03053092956543, 'learning_rate': 9.062253743104808e-06, 'epoch': 0.8222950565327851}
2024-12-25 00:23:35,341 - INFO - Training Logs at step 5400: {'loss': 22.4053, 'grad_norm': 12.3399076461792, 'learning_rate': 8.668242710795903e-06, 'epoch': 0.8300525570661133}
2024-12-25 00:24:16,961 - INFO - Training Logs at step 5450: {'loss': 21.9697, 'grad_norm': 18.041322708129883, 'learning_rate': 8.274231678486997e-06, 'epoch': 0.8378100575994415}
2024-12-25 00:24:57,904 - INFO - Training Logs at step 5500: {'loss': 22.5127, 'grad_norm': 17.47605323791504, 'learning_rate': 7.880220646178094e-06, 'epoch': 0.8455675581327696}
2024-12-25 00:25:38,766 - INFO - Training Logs at step 5550: {'loss': 22.4302, 'grad_norm': 15.788542747497559, 'learning_rate': 7.486209613869188e-06, 'epoch': 0.8533250586660978}
2024-12-25 00:26:19,964 - INFO - Training Logs at step 5600: {'loss': 22.2595, 'grad_norm': 17.774370193481445, 'learning_rate': 7.092198581560285e-06, 'epoch': 0.8610825591994259}
2024-12-25 00:27:01,491 - INFO - Training Logs at step 5650: {'loss': 22.2106, 'grad_norm': 14.862144470214844, 'learning_rate': 6.698187549251379e-06, 'epoch': 0.8688400597327541}
2024-12-25 00:27:42,811 - INFO - Training Logs at step 5700: {'loss': 22.287, 'grad_norm': 10.688333511352539, 'learning_rate': 6.304176516942474e-06, 'epoch': 0.8765975602660823}
2024-12-25 00:28:23,996 - INFO - Training Logs at step 5750: {'loss': 22.3034, 'grad_norm': 18.070392608642578, 'learning_rate': 5.91016548463357e-06, 'epoch': 0.8843550607994104}
2024-12-25 00:29:05,142 - INFO - Training Logs at step 5800: {'loss': 22.7703, 'grad_norm': 15.013510704040527, 'learning_rate': 5.516154452324665e-06, 'epoch': 0.8921125613327386}
2024-12-25 00:29:46,520 - INFO - Training Logs at step 5850: {'loss': 22.502, 'grad_norm': 14.303420066833496, 'learning_rate': 5.122143420015761e-06, 'epoch': 0.8998700618660668}
2024-12-25 00:30:28,246 - INFO - Training Logs at step 5900: {'loss': 22.5289, 'grad_norm': 18.694034576416016, 'learning_rate': 4.728132387706856e-06, 'epoch': 0.9076275623993949}
2024-12-25 00:31:09,339 - INFO - Training Logs at step 5950: {'loss': 22.4462, 'grad_norm': 15.438840866088867, 'learning_rate': 4.3341213553979515e-06, 'epoch': 0.915385062932723}
2024-12-25 00:31:50,638 - INFO - Training Logs at step 6000: {'loss': 22.4067, 'grad_norm': 13.533385276794434, 'learning_rate': 3.940110323089047e-06, 'epoch': 0.9231425634660513}
2024-12-25 00:32:32,127 - INFO - Training Logs at step 6050: {'loss': 22.2027, 'grad_norm': 14.649181365966797, 'learning_rate': 3.5460992907801423e-06, 'epoch': 0.9309000639993794}
2024-12-25 00:33:13,032 - INFO - Training Logs at step 6100: {'loss': 22.3202, 'grad_norm': 17.29821014404297, 'learning_rate': 3.152088258471237e-06, 'epoch': 0.9386575645327075}
2024-12-25 00:33:54,082 - INFO - Training Logs at step 6150: {'loss': 22.0952, 'grad_norm': 14.66030502319336, 'learning_rate': 2.7580772261623327e-06, 'epoch': 0.9464150650660357}
2024-12-25 00:34:35,072 - INFO - Training Logs at step 6200: {'loss': 22.1191, 'grad_norm': 15.723868370056152, 'learning_rate': 2.364066193853428e-06, 'epoch': 0.9541725655993639}
2024-12-25 00:35:16,129 - INFO - Training Logs at step 6250: {'loss': 22.5727, 'grad_norm': 18.875110626220703, 'learning_rate': 1.9700551615445235e-06, 'epoch': 0.961930066132692}
2024-12-25 00:35:57,373 - INFO - Training Logs at step 6300: {'loss': 22.1815, 'grad_norm': 14.184405326843262, 'learning_rate': 1.5760441292356184e-06, 'epoch': 0.9696875666660202}
2024-12-25 00:36:38,531 - INFO - Training Logs at step 6350: {'loss': 21.9586, 'grad_norm': 15.760008811950684, 'learning_rate': 1.182033096926714e-06, 'epoch': 0.9774450671993483}
2024-12-25 00:37:20,089 - INFO - Training Logs at step 6400: {'loss': 22.1253, 'grad_norm': 17.901561737060547, 'learning_rate': 7.880220646178092e-07, 'epoch': 0.9852025677326766}
2024-12-25 00:37:58,723 - INFO - Model ytu-ce-cosmos/turkish-gpt2-medium ince ayar işlemi başarıyla tamamlandı.
2024-12-25 00:37:58,723 - INFO - Model ve tokenizer kaydediliyor: ./models/turkish-gpt2-medium_v2
2024-12-25 00:38:04,204 - INFO - Model ./models/turkish-gpt2-medium_v2 dizinine başarıyla kaydedildi.
2024-12-25 00:38:04,204 - INFO - [SUCCESS] Eğitim tamamlandı: Model=ytu-ce-cosmos/turkish-gpt2-medium, Dataset=v2
