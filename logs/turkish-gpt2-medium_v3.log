2024-12-24 23:22:16,292 - INFO - [INFO] Eğitim başlıyor: Model=ytu-ce-cosmos/turkish-gpt2-medium, Dataset=v3
2024-12-24 23:22:22,329 - INFO - Dataset başarıyla yüklendi: v3
2024-12-24 23:22:22,329 - INFO - Model ve tokenizer yükleniyor: ytu-ce-cosmos/turkish-gpt2-medium
2024-12-24 23:22:24,235 - INFO - Model ve tokenizer başarıyla yüklendi: ytu-ce-cosmos/turkish-gpt2-medium
2024-12-24 23:22:24,235 - INFO - Model ytu-ce-cosmos/turkish-gpt2-medium ince ayar işlemi başlatılıyor.
2024-12-24 23:22:48,357 - INFO - Dataset başarıyla işlendi: max_seq_length=256
2024-12-24 23:24:14,359 - INFO - Training Logs at step 100: {'loss': 28.807, 'grad_norm': 10.086130142211914, 'learning_rate': 2.35e-05, 'epoch': 0.005857115663391563}
2024-12-24 23:24:56,071 - INFO - Training Logs at step 150: {'loss': 26.8673, 'grad_norm': 20.59200668334961, 'learning_rate': 4.85e-05, 'epoch': 0.011714231326783125}
2024-12-24 23:25:39,113 - INFO - Training Logs at step 200: {'loss': 25.5199, 'grad_norm': 21.10330581665039, 'learning_rate': 4.972143195827407e-05, 'epoch': 0.017571346990174687}
2024-12-24 23:26:20,282 - INFO - Training Logs at step 250: {'loss': 25.3588, 'grad_norm': 14.6298246383667, 'learning_rate': 4.942508297771456e-05, 'epoch': 0.02342846265356625}
2024-12-24 23:27:01,569 - INFO - Training Logs at step 300: {'loss': 24.4862, 'grad_norm': 19.184181213378906, 'learning_rate': 4.912873399715505e-05, 'epoch': 0.029285578316957814}
2024-12-24 23:27:43,635 - INFO - Training Logs at step 350: {'loss': 25.0915, 'grad_norm': 17.677383422851562, 'learning_rate': 4.8832385016595546e-05, 'epoch': 0.035142693980349374}
2024-12-24 23:28:26,476 - INFO - Training Logs at step 400: {'loss': 24.3185, 'grad_norm': 14.069574356079102, 'learning_rate': 4.853603603603604e-05, 'epoch': 0.04099980964374094}
2024-12-24 23:29:08,013 - INFO - Training Logs at step 450: {'loss': 24.3346, 'grad_norm': 14.843180656433105, 'learning_rate': 4.823968705547653e-05, 'epoch': 0.0468569253071325}
2024-12-24 23:29:51,468 - INFO - Training Logs at step 500: {'loss': 24.4244, 'grad_norm': 16.324913024902344, 'learning_rate': 4.7943338074917025e-05, 'epoch': 0.05271404097052407}
2024-12-24 23:30:36,027 - INFO - Training Logs at step 550: {'loss': 24.2119, 'grad_norm': 13.693477630615234, 'learning_rate': 4.764698909435752e-05, 'epoch': 0.05857115663391563}
2024-12-24 23:31:19,502 - INFO - Training Logs at step 600: {'loss': 24.0521, 'grad_norm': 11.79138469696045, 'learning_rate': 4.735064011379801e-05, 'epoch': 0.06442827229730719}
2024-12-24 23:32:02,816 - INFO - Training Logs at step 650: {'loss': 23.8014, 'grad_norm': 13.603755950927734, 'learning_rate': 4.70542911332385e-05, 'epoch': 0.07028538796069875}
2024-12-24 23:32:46,575 - INFO - Training Logs at step 700: {'loss': 24.0434, 'grad_norm': 12.607246398925781, 'learning_rate': 4.675794215267899e-05, 'epoch': 0.07614250362409032}
2024-12-24 23:33:28,342 - INFO - Training Logs at step 750: {'loss': 24.0121, 'grad_norm': 12.489835739135742, 'learning_rate': 4.646159317211949e-05, 'epoch': 0.08199961928748188}
2024-12-24 23:34:10,174 - INFO - Training Logs at step 800: {'loss': 23.8363, 'grad_norm': 18.85237693786621, 'learning_rate': 4.616524419155998e-05, 'epoch': 0.08785673495087344}
2024-12-24 23:34:51,886 - INFO - Training Logs at step 850: {'loss': 23.8523, 'grad_norm': 12.09627628326416, 'learning_rate': 4.586889521100048e-05, 'epoch': 0.093713850614265}
2024-12-24 23:35:33,977 - INFO - Training Logs at step 900: {'loss': 23.9702, 'grad_norm': 13.73943042755127, 'learning_rate': 4.557254623044097e-05, 'epoch': 0.09957096627765656}
2024-12-24 23:36:16,653 - INFO - Training Logs at step 950: {'loss': 23.8613, 'grad_norm': 11.957332611083984, 'learning_rate': 4.5276197249881466e-05, 'epoch': 0.10542808194104814}
2024-12-24 23:36:58,675 - INFO - Training Logs at step 1000: {'loss': 24.0918, 'grad_norm': 13.473947525024414, 'learning_rate': 4.4979848269321956e-05, 'epoch': 0.1112851976044397}
2024-12-24 23:37:40,563 - INFO - Training Logs at step 1050: {'loss': 23.7381, 'grad_norm': 16.011104583740234, 'learning_rate': 4.4683499288762447e-05, 'epoch': 0.11714231326783126}
2024-12-24 23:38:22,519 - INFO - Training Logs at step 1100: {'loss': 23.9482, 'grad_norm': 9.3436861038208, 'learning_rate': 4.4387150308202944e-05, 'epoch': 0.12299942893122282}
2024-12-24 23:39:04,334 - INFO - Training Logs at step 1150: {'loss': 23.6298, 'grad_norm': 12.993062019348145, 'learning_rate': 4.4090801327643434e-05, 'epoch': 0.12885654459461438}
2024-12-24 23:39:46,351 - INFO - Training Logs at step 1200: {'loss': 23.4226, 'grad_norm': 14.228926658630371, 'learning_rate': 4.379445234708393e-05, 'epoch': 0.13471366025800594}
2024-12-24 23:40:29,116 - INFO - Training Logs at step 1250: {'loss': 23.7232, 'grad_norm': 23.8985595703125, 'learning_rate': 4.349810336652442e-05, 'epoch': 0.1405707759213975}
2024-12-24 23:41:10,877 - INFO - Training Logs at step 1300: {'loss': 23.9456, 'grad_norm': 12.940031051635742, 'learning_rate': 4.320175438596491e-05, 'epoch': 0.14642789158478908}
2024-12-24 23:41:52,725 - INFO - Training Logs at step 1350: {'loss': 23.8222, 'grad_norm': 15.767252922058105, 'learning_rate': 4.290540540540541e-05, 'epoch': 0.15228500724818064}
2024-12-24 23:42:34,371 - INFO - Training Logs at step 1400: {'loss': 23.6462, 'grad_norm': 14.59673023223877, 'learning_rate': 4.26090564248459e-05, 'epoch': 0.1581421229115722}
2024-12-24 23:43:16,193 - INFO - Training Logs at step 1450: {'loss': 23.3182, 'grad_norm': 15.043213844299316, 'learning_rate': 4.231270744428639e-05, 'epoch': 0.16399923857496376}
2024-12-24 23:43:58,433 - INFO - Training Logs at step 1500: {'loss': 23.764, 'grad_norm': 12.975764274597168, 'learning_rate': 4.201635846372689e-05, 'epoch': 0.16985635423835532}
2024-12-24 23:44:41,745 - INFO - Training Logs at step 1550: {'loss': 23.943, 'grad_norm': 15.676530838012695, 'learning_rate': 4.1720009483167385e-05, 'epoch': 0.17571346990174688}
2024-12-24 23:45:24,161 - INFO - Training Logs at step 1600: {'loss': 23.7417, 'grad_norm': 14.716118812561035, 'learning_rate': 4.1423660502607875e-05, 'epoch': 0.18157058556513844}
2024-12-24 23:46:07,850 - INFO - Training Logs at step 1650: {'loss': 23.5216, 'grad_norm': 16.447404861450195, 'learning_rate': 4.1127311522048366e-05, 'epoch': 0.18742770122853}
2024-12-24 23:46:50,570 - INFO - Training Logs at step 1700: {'loss': 23.4855, 'grad_norm': 10.346281051635742, 'learning_rate': 4.083096254148886e-05, 'epoch': 0.19328481689192156}
2024-12-24 23:47:32,512 - INFO - Training Logs at step 1750: {'loss': 23.2552, 'grad_norm': 11.961613655090332, 'learning_rate': 4.0534613560929353e-05, 'epoch': 0.19914193255531312}
2024-12-24 23:48:14,243 - INFO - Training Logs at step 1800: {'loss': 23.9603, 'grad_norm': 16.555749893188477, 'learning_rate': 4.0238264580369844e-05, 'epoch': 0.2049990482187047}
2024-12-24 23:48:55,405 - INFO - Training Logs at step 1850: {'loss': 23.6591, 'grad_norm': 16.548582077026367, 'learning_rate': 3.9941915599810334e-05, 'epoch': 0.21085616388209627}
2024-12-24 23:49:36,360 - INFO - Training Logs at step 1900: {'loss': 23.5436, 'grad_norm': 15.847329139709473, 'learning_rate': 3.964556661925083e-05, 'epoch': 0.21671327954548783}
2024-12-24 23:50:17,146 - INFO - Training Logs at step 1950: {'loss': 23.4635, 'grad_norm': 13.3972749710083, 'learning_rate': 3.934921763869133e-05, 'epoch': 0.2225703952088794}
2024-12-24 23:50:58,416 - INFO - Training Logs at step 2000: {'loss': 23.4316, 'grad_norm': 13.734915733337402, 'learning_rate': 3.905286865813182e-05, 'epoch': 0.22842751087227095}
2024-12-24 23:51:39,741 - INFO - Training Logs at step 2050: {'loss': 23.8204, 'grad_norm': 14.468334197998047, 'learning_rate': 3.875651967757231e-05, 'epoch': 0.2342846265356625}
2024-12-24 23:52:21,067 - INFO - Training Logs at step 2100: {'loss': 23.8453, 'grad_norm': 16.848060607910156, 'learning_rate': 3.846017069701281e-05, 'epoch': 0.24014174219905407}
2024-12-24 23:53:02,670 - INFO - Training Logs at step 2150: {'loss': 23.1685, 'grad_norm': 17.656002044677734, 'learning_rate': 3.81638217164533e-05, 'epoch': 0.24599885786244563}
2024-12-24 23:53:44,449 - INFO - Training Logs at step 2200: {'loss': 23.8426, 'grad_norm': 13.30825138092041, 'learning_rate': 3.786747273589379e-05, 'epoch': 0.2518559735258372}
2024-12-24 23:54:25,612 - INFO - Training Logs at step 2250: {'loss': 23.5654, 'grad_norm': 11.412142753601074, 'learning_rate': 3.757112375533428e-05, 'epoch': 0.25771308918922875}
2024-12-24 23:55:06,643 - INFO - Training Logs at step 2300: {'loss': 23.6655, 'grad_norm': 25.694477081298828, 'learning_rate': 3.727477477477478e-05, 'epoch': 0.26357020485262034}
2024-12-24 23:55:48,499 - INFO - Training Logs at step 2350: {'loss': 23.242, 'grad_norm': 15.71129035949707, 'learning_rate': 3.697842579421527e-05, 'epoch': 0.26942732051601187}
2024-12-24 23:56:31,447 - INFO - Training Logs at step 2400: {'loss': 23.13, 'grad_norm': 12.792222023010254, 'learning_rate': 3.668207681365576e-05, 'epoch': 0.27528443617940346}
2024-12-24 23:57:14,447 - INFO - Training Logs at step 2450: {'loss': 23.0639, 'grad_norm': 14.170748710632324, 'learning_rate': 3.6385727833096254e-05, 'epoch': 0.281141551842795}
2024-12-24 23:57:56,665 - INFO - Training Logs at step 2500: {'loss': 23.5609, 'grad_norm': 17.826690673828125, 'learning_rate': 3.608937885253675e-05, 'epoch': 0.2869986675061866}
2024-12-24 23:58:38,196 - INFO - Training Logs at step 2550: {'loss': 23.2698, 'grad_norm': 13.506473541259766, 'learning_rate': 3.579302987197724e-05, 'epoch': 0.29285578316957817}
2024-12-24 23:59:19,469 - INFO - Training Logs at step 2600: {'loss': 23.695, 'grad_norm': 16.73908233642578, 'learning_rate': 3.549668089141773e-05, 'epoch': 0.2987128988329697}
2024-12-25 00:00:01,196 - INFO - Training Logs at step 2650: {'loss': 23.4182, 'grad_norm': 16.1115665435791, 'learning_rate': 3.520033191085823e-05, 'epoch': 0.3045700144963613}
2024-12-25 00:00:42,935 - INFO - Training Logs at step 2700: {'loss': 23.5808, 'grad_norm': 16.437170028686523, 'learning_rate': 3.4903982930298726e-05, 'epoch': 0.3104271301597528}
2024-12-25 00:01:25,303 - INFO - Training Logs at step 2750: {'loss': 23.9644, 'grad_norm': 15.064396858215332, 'learning_rate': 3.4607633949739217e-05, 'epoch': 0.3162842458231444}
2024-12-25 00:02:07,555 - INFO - Training Logs at step 2800: {'loss': 23.5187, 'grad_norm': 20.705875396728516, 'learning_rate': 3.43172119487909e-05, 'epoch': 0.32214136148653594}
2024-12-25 00:02:49,232 - INFO - Training Logs at step 2850: {'loss': 22.9231, 'grad_norm': 10.810660362243652, 'learning_rate': 3.4020862968231396e-05, 'epoch': 0.3279984771499275}
2024-12-25 00:03:31,513 - INFO - Training Logs at step 2900: {'loss': 23.1239, 'grad_norm': 14.57146167755127, 'learning_rate': 3.3724513987671886e-05, 'epoch': 0.33385559281331906}
2024-12-25 00:04:13,178 - INFO - Training Logs at step 2950: {'loss': 22.9022, 'grad_norm': 17.19649887084961, 'learning_rate': 3.342816500711238e-05, 'epoch': 0.33971270847671065}
2024-12-25 00:04:55,064 - INFO - Training Logs at step 3000: {'loss': 23.0779, 'grad_norm': 16.133405685424805, 'learning_rate': 3.313181602655287e-05, 'epoch': 0.34556982414010223}
2024-12-25 00:05:38,150 - INFO - Training Logs at step 3050: {'loss': 22.9633, 'grad_norm': 12.315013885498047, 'learning_rate': 3.2835467045993365e-05, 'epoch': 0.35142693980349377}
2024-12-25 00:06:20,218 - INFO - Training Logs at step 3100: {'loss': 23.9881, 'grad_norm': 14.66158676147461, 'learning_rate': 3.2539118065433855e-05, 'epoch': 0.35728405546688535}
2024-12-25 00:07:03,037 - INFO - Training Logs at step 3150: {'loss': 23.1335, 'grad_norm': 17.103532791137695, 'learning_rate': 3.2242769084874345e-05, 'epoch': 0.3631411711302769}
2024-12-25 00:07:45,546 - INFO - Training Logs at step 3200: {'loss': 23.1915, 'grad_norm': 12.720144271850586, 'learning_rate': 3.194642010431484e-05, 'epoch': 0.3689982867936685}
2024-12-25 00:08:27,219 - INFO - Training Logs at step 3250: {'loss': 23.2566, 'grad_norm': 17.15395736694336, 'learning_rate': 3.165007112375534e-05, 'epoch': 0.37485540245706}
2024-12-25 00:09:09,084 - INFO - Training Logs at step 3300: {'loss': 23.0505, 'grad_norm': 15.72240161895752, 'learning_rate': 3.135372214319583e-05, 'epoch': 0.3807125181204516}
2024-12-25 00:09:50,738 - INFO - Training Logs at step 3350: {'loss': 23.2326, 'grad_norm': 16.673749923706055, 'learning_rate': 3.105737316263632e-05, 'epoch': 0.3865696337838431}
2024-12-25 00:10:32,118 - INFO - Training Logs at step 3400: {'loss': 22.9401, 'grad_norm': 17.575023651123047, 'learning_rate': 3.076102418207681e-05, 'epoch': 0.3924267494472347}
2024-12-25 00:11:13,602 - INFO - Training Logs at step 3450: {'loss': 23.5046, 'grad_norm': 15.11967658996582, 'learning_rate': 3.0464675201517305e-05, 'epoch': 0.39828386511062625}
2024-12-25 00:11:55,494 - INFO - Training Logs at step 3500: {'loss': 23.1235, 'grad_norm': 13.619425773620605, 'learning_rate': 3.01683262209578e-05, 'epoch': 0.40414098077401783}
2024-12-25 00:12:36,793 - INFO - Training Logs at step 3550: {'loss': 23.1771, 'grad_norm': 16.241397857666016, 'learning_rate': 2.9871977240398296e-05, 'epoch': 0.4099980964374094}
2024-12-25 00:13:18,919 - INFO - Training Logs at step 3600: {'loss': 22.7854, 'grad_norm': 17.093088150024414, 'learning_rate': 2.957562825983879e-05, 'epoch': 0.41585521210080095}
2024-12-25 00:14:00,708 - INFO - Training Logs at step 3650: {'loss': 23.1886, 'grad_norm': 15.933717727661133, 'learning_rate': 2.927927927927928e-05, 'epoch': 0.42171232776419254}
2024-12-25 00:14:41,969 - INFO - Training Logs at step 3700: {'loss': 23.3376, 'grad_norm': 14.595908164978027, 'learning_rate': 2.8982930298719774e-05, 'epoch': 0.4275694434275841}
2024-12-25 00:15:23,075 - INFO - Training Logs at step 3750: {'loss': 22.9918, 'grad_norm': 12.552040100097656, 'learning_rate': 2.8686581318160265e-05, 'epoch': 0.43342655909097566}
2024-12-25 00:16:04,290 - INFO - Training Logs at step 3800: {'loss': 23.3428, 'grad_norm': 16.62830352783203, 'learning_rate': 2.839023233760076e-05, 'epoch': 0.4392836747543672}
2024-12-25 00:16:45,537 - INFO - Training Logs at step 3850: {'loss': 22.6125, 'grad_norm': 14.84919261932373, 'learning_rate': 2.8093883357041252e-05, 'epoch': 0.4451407904177588}
2024-12-25 00:17:26,971 - INFO - Training Logs at step 3900: {'loss': 23.3266, 'grad_norm': 14.566971778869629, 'learning_rate': 2.779753437648175e-05, 'epoch': 0.4509979060811503}
2024-12-25 00:18:08,985 - INFO - Training Logs at step 3950: {'loss': 23.2485, 'grad_norm': 16.639324188232422, 'learning_rate': 2.750118539592224e-05, 'epoch': 0.4568550217445419}
2024-12-25 00:18:50,225 - INFO - Training Logs at step 4000: {'loss': 23.3971, 'grad_norm': 15.610884666442871, 'learning_rate': 2.7204836415362734e-05, 'epoch': 0.4627121374079335}
2024-12-25 00:19:31,488 - INFO - Training Logs at step 4050: {'loss': 23.4311, 'grad_norm': 14.849404335021973, 'learning_rate': 2.6908487434803224e-05, 'epoch': 0.468569253071325}
2024-12-25 00:20:12,710 - INFO - Training Logs at step 4100: {'loss': 22.9188, 'grad_norm': 14.430645942687988, 'learning_rate': 2.6612138454243718e-05, 'epoch': 0.4744263687347166}
2024-12-25 00:20:53,856 - INFO - Training Logs at step 4150: {'loss': 22.9375, 'grad_norm': 12.62808609008789, 'learning_rate': 2.6315789473684212e-05, 'epoch': 0.48028348439810814}
2024-12-25 00:21:34,959 - INFO - Training Logs at step 4200: {'loss': 22.9717, 'grad_norm': 17.776601791381836, 'learning_rate': 2.6019440493124702e-05, 'epoch': 0.48614060006149973}
2024-12-25 00:22:16,119 - INFO - Training Logs at step 4250: {'loss': 22.9247, 'grad_norm': 11.104964256286621, 'learning_rate': 2.57230915125652e-05, 'epoch': 0.49199771572489126}
2024-12-25 00:22:57,921 - INFO - Training Logs at step 4300: {'loss': 22.3772, 'grad_norm': 15.435081481933594, 'learning_rate': 2.5426742532005693e-05, 'epoch': 0.49785483138828285}
2024-12-25 00:23:39,711 - INFO - Training Logs at step 4350: {'loss': 22.6673, 'grad_norm': 15.37389087677002, 'learning_rate': 2.5130393551446184e-05, 'epoch': 0.5037119470516744}
2024-12-25 00:24:20,989 - INFO - Training Logs at step 4400: {'loss': 23.0897, 'grad_norm': 23.950763702392578, 'learning_rate': 2.4834044570886678e-05, 'epoch': 0.509569062715066}
2024-12-25 00:25:02,220 - INFO - Training Logs at step 4450: {'loss': 23.507, 'grad_norm': 15.593484878540039, 'learning_rate': 2.453769559032717e-05, 'epoch': 0.5154261783784575}
2024-12-25 00:25:43,590 - INFO - Training Logs at step 4500: {'loss': 22.6182, 'grad_norm': 14.604273796081543, 'learning_rate': 2.4241346609767662e-05, 'epoch': 0.5212832940418491}
2024-12-25 00:26:24,765 - INFO - Training Logs at step 4550: {'loss': 23.0007, 'grad_norm': 12.247129440307617, 'learning_rate': 2.3944997629208156e-05, 'epoch': 0.5271404097052407}
2024-12-25 00:27:06,106 - INFO - Training Logs at step 4600: {'loss': 22.6151, 'grad_norm': 17.922744750976562, 'learning_rate': 2.364864864864865e-05, 'epoch': 0.5329975253686322}
2024-12-25 00:27:47,400 - INFO - Training Logs at step 4650: {'loss': 23.31, 'grad_norm': 16.794780731201172, 'learning_rate': 2.3352299668089144e-05, 'epoch': 0.5388546410320237}
2024-12-25 00:28:29,149 - INFO - Training Logs at step 4700: {'loss': 22.9091, 'grad_norm': 18.17304801940918, 'learning_rate': 2.3055950687529637e-05, 'epoch': 0.5447117566954154}
2024-12-25 00:29:10,809 - INFO - Training Logs at step 4750: {'loss': 22.9955, 'grad_norm': 10.623878479003906, 'learning_rate': 2.275960170697013e-05, 'epoch': 0.5505688723588069}
2024-12-25 00:29:52,141 - INFO - Training Logs at step 4800: {'loss': 23.1847, 'grad_norm': 16.61818504333496, 'learning_rate': 2.246325272641062e-05, 'epoch': 0.5564259880221984}
2024-12-25 00:30:33,250 - INFO - Training Logs at step 4850: {'loss': 23.3024, 'grad_norm': 12.880220413208008, 'learning_rate': 2.2166903745851115e-05, 'epoch': 0.56228310368559}
2024-12-25 00:31:14,233 - INFO - Training Logs at step 4900: {'loss': 22.8258, 'grad_norm': 23.71190643310547, 'learning_rate': 2.187055476529161e-05, 'epoch': 0.5681402193489816}
2024-12-25 00:31:55,167 - INFO - Training Logs at step 4950: {'loss': 23.0202, 'grad_norm': 15.667341232299805, 'learning_rate': 2.1574205784732103e-05, 'epoch': 0.5739973350123732}
2024-12-25 00:32:36,093 - INFO - Training Logs at step 5000: {'loss': 22.7274, 'grad_norm': 11.197875022888184, 'learning_rate': 2.1277856804172594e-05, 'epoch': 0.5798544506757647}
2024-12-25 00:33:17,124 - INFO - Training Logs at step 5050: {'loss': 22.7428, 'grad_norm': 14.610239028930664, 'learning_rate': 2.0981507823613087e-05, 'epoch': 0.5857115663391563}
2024-12-25 00:33:57,921 - INFO - Training Logs at step 5100: {'loss': 22.931, 'grad_norm': 14.299078941345215, 'learning_rate': 2.068515884305358e-05, 'epoch': 0.5915686820025479}
2024-12-25 00:34:39,184 - INFO - Training Logs at step 5150: {'loss': 22.5221, 'grad_norm': 12.98194408416748, 'learning_rate': 2.0388809862494075e-05, 'epoch': 0.5974257976659394}
2024-12-25 00:35:20,484 - INFO - Training Logs at step 5200: {'loss': 22.7835, 'grad_norm': 20.38932228088379, 'learning_rate': 2.0092460881934566e-05, 'epoch': 0.6032829133293309}
2024-12-25 00:36:01,324 - INFO - Training Logs at step 5250: {'loss': 22.5902, 'grad_norm': 13.232556343078613, 'learning_rate': 1.9796111901375063e-05, 'epoch': 0.6091400289927226}
2024-12-25 00:36:42,079 - INFO - Training Logs at step 5300: {'loss': 23.2385, 'grad_norm': 14.479156494140625, 'learning_rate': 1.9499762920815553e-05, 'epoch': 0.6149971446561141}
2024-12-25 00:37:22,967 - INFO - Training Logs at step 5350: {'loss': 22.8744, 'grad_norm': 15.230917930603027, 'learning_rate': 1.9203413940256047e-05, 'epoch': 0.6208542603195056}
2024-12-25 00:38:03,798 - INFO - Training Logs at step 5400: {'loss': 22.447, 'grad_norm': 13.087749481201172, 'learning_rate': 1.8907064959696537e-05, 'epoch': 0.6267113759828972}
2024-12-25 00:38:44,804 - INFO - Training Logs at step 5450: {'loss': 23.1175, 'grad_norm': 14.560725212097168, 'learning_rate': 1.8610715979137035e-05, 'epoch': 0.6325684916462888}
2024-12-25 00:39:25,686 - INFO - Training Logs at step 5500: {'loss': 22.8398, 'grad_norm': 18.776214599609375, 'learning_rate': 1.8314366998577525e-05, 'epoch': 0.6384256073096803}
2024-12-25 00:40:06,632 - INFO - Training Logs at step 5550: {'loss': 22.9288, 'grad_norm': 21.870058059692383, 'learning_rate': 1.801801801801802e-05, 'epoch': 0.6442827229730719}
2024-12-25 00:40:47,589 - INFO - Training Logs at step 5600: {'loss': 23.1533, 'grad_norm': 18.309396743774414, 'learning_rate': 1.7721669037458513e-05, 'epoch': 0.6501398386364635}
2024-12-25 00:41:29,293 - INFO - Training Logs at step 5650: {'loss': 22.9993, 'grad_norm': 21.5757999420166, 'learning_rate': 1.7431247036510195e-05, 'epoch': 0.655996954299855}
2024-12-25 00:42:10,522 - INFO - Training Logs at step 5700: {'loss': 23.3327, 'grad_norm': 13.298369407653809, 'learning_rate': 1.713489805595069e-05, 'epoch': 0.6618540699632466}
2024-12-25 00:42:51,507 - INFO - Training Logs at step 5750: {'loss': 22.8554, 'grad_norm': 15.84721851348877, 'learning_rate': 1.683854907539118e-05, 'epoch': 0.6677111856266381}
2024-12-25 00:43:32,357 - INFO - Training Logs at step 5800: {'loss': 22.5239, 'grad_norm': 18.03034782409668, 'learning_rate': 1.6542200094831677e-05, 'epoch': 0.6735683012900298}
2024-12-25 00:44:13,195 - INFO - Training Logs at step 5850: {'loss': 23.2144, 'grad_norm': 12.306842803955078, 'learning_rate': 1.6245851114272167e-05, 'epoch': 0.6794254169534213}
2024-12-25 00:44:54,002 - INFO - Training Logs at step 5900: {'loss': 22.9944, 'grad_norm': 16.483776092529297, 'learning_rate': 1.594950213371266e-05, 'epoch': 0.6852825326168128}
2024-12-25 00:45:34,832 - INFO - Training Logs at step 5950: {'loss': 22.7192, 'grad_norm': 21.69363784790039, 'learning_rate': 1.5659080132764346e-05, 'epoch': 0.6911396482802045}
2024-12-25 00:46:15,760 - INFO - Training Logs at step 6000: {'loss': 23.1993, 'grad_norm': 16.736534118652344, 'learning_rate': 1.5362731152204837e-05, 'epoch': 0.696996763943596}
2024-12-25 00:46:56,679 - INFO - Training Logs at step 6050: {'loss': 22.8867, 'grad_norm': 15.037812232971191, 'learning_rate': 1.5066382171645329e-05, 'epoch': 0.7028538796069875}
2024-12-25 00:47:38,039 - INFO - Training Logs at step 6100: {'loss': 22.5417, 'grad_norm': 15.584754943847656, 'learning_rate': 1.4770033191085825e-05, 'epoch': 0.7087109952703791}
2024-12-25 00:48:19,490 - INFO - Training Logs at step 6150: {'loss': 23.1775, 'grad_norm': 13.674662590026855, 'learning_rate': 1.4473684210526317e-05, 'epoch': 0.7145681109337707}
2024-12-25 00:49:00,627 - INFO - Training Logs at step 6200: {'loss': 22.9305, 'grad_norm': 16.72412109375, 'learning_rate': 1.4177335229966809e-05, 'epoch': 0.7204252265971622}
2024-12-25 00:49:41,644 - INFO - Training Logs at step 6250: {'loss': 23.0809, 'grad_norm': 16.328289031982422, 'learning_rate': 1.3880986249407301e-05, 'epoch': 0.7262823422605538}
2024-12-25 00:50:22,553 - INFO - Training Logs at step 6300: {'loss': 23.2874, 'grad_norm': 22.32496452331543, 'learning_rate': 1.3584637268847797e-05, 'epoch': 0.7321394579239453}
2024-12-25 00:51:03,514 - INFO - Training Logs at step 6350: {'loss': 22.9316, 'grad_norm': 14.641459465026855, 'learning_rate': 1.3288288288288289e-05, 'epoch': 0.737996573587337}
2024-12-25 00:51:44,445 - INFO - Training Logs at step 6400: {'loss': 23.0228, 'grad_norm': 16.82297134399414, 'learning_rate': 1.299193930772878e-05, 'epoch': 0.7438536892507285}
2024-12-25 00:52:25,294 - INFO - Training Logs at step 6450: {'loss': 22.6969, 'grad_norm': 13.712493896484375, 'learning_rate': 1.2695590327169275e-05, 'epoch': 0.74971080491412}
2024-12-25 00:53:06,375 - INFO - Training Logs at step 6500: {'loss': 23.0217, 'grad_norm': 15.39010238647461, 'learning_rate': 1.2399241346609767e-05, 'epoch': 0.7555679205775117}
2024-12-25 00:53:47,633 - INFO - Training Logs at step 6550: {'loss': 22.8447, 'grad_norm': 17.269058227539062, 'learning_rate': 1.210289236605026e-05, 'epoch': 0.7614250362409032}
2024-12-25 00:54:29,067 - INFO - Training Logs at step 6600: {'loss': 22.9966, 'grad_norm': 17.742305755615234, 'learning_rate': 1.1806543385490754e-05, 'epoch': 0.7672821519042947}
2024-12-25 00:55:10,163 - INFO - Training Logs at step 6650: {'loss': 23.088, 'grad_norm': 12.17941951751709, 'learning_rate': 1.1510194404931247e-05, 'epoch': 0.7731392675676863}
2024-12-25 00:55:51,296 - INFO - Training Logs at step 6700: {'loss': 23.2934, 'grad_norm': 12.791657447814941, 'learning_rate': 1.121384542437174e-05, 'epoch': 0.7789963832310779}
2024-12-25 00:56:32,264 - INFO - Training Logs at step 6750: {'loss': 22.5057, 'grad_norm': 10.39803695678711, 'learning_rate': 1.0917496443812234e-05, 'epoch': 0.7848534988944694}
2024-12-25 00:57:13,083 - INFO - Training Logs at step 6800: {'loss': 23.2052, 'grad_norm': 17.338546752929688, 'learning_rate': 1.0621147463252726e-05, 'epoch': 0.790710614557861}
2024-12-25 00:57:54,140 - INFO - Training Logs at step 6850: {'loss': 23.0721, 'grad_norm': 14.544196128845215, 'learning_rate': 1.032479848269322e-05, 'epoch': 0.7965677302212525}
2024-12-25 00:58:35,316 - INFO - Training Logs at step 6900: {'loss': 23.0156, 'grad_norm': 19.726011276245117, 'learning_rate': 1.0028449502133714e-05, 'epoch': 0.8024248458846441}
2024-12-25 00:59:16,528 - INFO - Training Logs at step 6950: {'loss': 23.2217, 'grad_norm': 12.535383224487305, 'learning_rate': 9.732100521574206e-06, 'epoch': 0.8082819615480357}
2024-12-25 00:59:58,157 - INFO - Training Logs at step 7000: {'loss': 22.7949, 'grad_norm': 17.373315811157227, 'learning_rate': 9.4357515410147e-06, 'epoch': 0.8141390772114272}
2024-12-25 01:00:39,984 - INFO - Training Logs at step 7050: {'loss': 23.1015, 'grad_norm': 12.291091918945312, 'learning_rate': 9.139402560455194e-06, 'epoch': 0.8199961928748188}
2024-12-25 01:01:21,204 - INFO - Training Logs at step 7100: {'loss': 22.4276, 'grad_norm': 15.290163040161133, 'learning_rate': 8.843053579895686e-06, 'epoch': 0.8258533085382104}
2024-12-25 01:02:02,472 - INFO - Training Logs at step 7150: {'loss': 22.9749, 'grad_norm': 15.584451675415039, 'learning_rate': 8.54670459933618e-06, 'epoch': 0.8317104242016019}
2024-12-25 01:02:43,401 - INFO - Training Logs at step 7200: {'loss': 22.892, 'grad_norm': 15.10006332397461, 'learning_rate': 8.250355618776672e-06, 'epoch': 0.8375675398649934}
2024-12-25 01:03:24,875 - INFO - Training Logs at step 7250: {'loss': 22.9348, 'grad_norm': 13.581801414489746, 'learning_rate': 7.954006638217166e-06, 'epoch': 0.8434246555283851}
2024-12-25 01:04:06,069 - INFO - Training Logs at step 7300: {'loss': 23.1017, 'grad_norm': 26.460424423217773, 'learning_rate': 7.657657657657658e-06, 'epoch': 0.8492817711917766}
2024-12-25 01:04:47,175 - INFO - Training Logs at step 7350: {'loss': 22.6876, 'grad_norm': 15.426139831542969, 'learning_rate': 7.361308677098151e-06, 'epoch': 0.8551388868551681}
2024-12-25 01:05:28,498 - INFO - Training Logs at step 7400: {'loss': 22.6594, 'grad_norm': 13.220224380493164, 'learning_rate': 7.064959696538644e-06, 'epoch': 0.8609960025185598}
2024-12-25 01:06:10,272 - INFO - Training Logs at step 7450: {'loss': 22.7331, 'grad_norm': 15.016693115234375, 'learning_rate': 6.768610715979137e-06, 'epoch': 0.8668531181819513}
2024-12-25 01:06:51,280 - INFO - Training Logs at step 7500: {'loss': 22.7894, 'grad_norm': 16.878793716430664, 'learning_rate': 6.472261735419631e-06, 'epoch': 0.8727102338453429}
2024-12-25 01:07:32,444 - INFO - Training Logs at step 7550: {'loss': 22.6706, 'grad_norm': 18.896774291992188, 'learning_rate': 6.175912754860124e-06, 'epoch': 0.8785673495087344}
2024-12-25 01:08:13,770 - INFO - Training Logs at step 7600: {'loss': 22.6992, 'grad_norm': 17.218429565429688, 'learning_rate': 5.879563774300617e-06, 'epoch': 0.884424465172126}
2024-12-25 01:08:55,185 - INFO - Training Logs at step 7650: {'loss': 22.4237, 'grad_norm': 15.406370162963867, 'learning_rate': 5.58321479374111e-06, 'epoch': 0.8902815808355176}
2024-12-25 01:09:36,481 - INFO - Training Logs at step 7700: {'loss': 22.7988, 'grad_norm': 14.923982620239258, 'learning_rate': 5.2868658131816036e-06, 'epoch': 0.8961386964989091}
2024-12-25 01:10:17,850 - INFO - Training Logs at step 7750: {'loss': 22.899, 'grad_norm': 15.762907981872559, 'learning_rate': 4.9905168326220966e-06, 'epoch': 0.9019958121623006}
2024-12-25 01:10:59,433 - INFO - Training Logs at step 7800: {'loss': 22.7696, 'grad_norm': 10.611122131347656, 'learning_rate': 4.6941678520625895e-06, 'epoch': 0.9078529278256923}
2024-12-25 01:11:41,332 - INFO - Training Logs at step 7850: {'loss': 22.6123, 'grad_norm': 14.603338241577148, 'learning_rate': 4.3978188715030825e-06, 'epoch': 0.9137100434890838}
2024-12-25 01:12:22,155 - INFO - Training Logs at step 7900: {'loss': 22.9499, 'grad_norm': 14.05195140838623, 'learning_rate': 4.1014698909435755e-06, 'epoch': 0.9195671591524753}
2024-12-25 01:13:03,017 - INFO - Training Logs at step 7950: {'loss': 22.9927, 'grad_norm': 18.186248779296875, 'learning_rate': 3.8051209103840685e-06, 'epoch': 0.925424274815867}
2024-12-25 01:13:44,241 - INFO - Training Logs at step 8000: {'loss': 22.8252, 'grad_norm': 14.293703079223633, 'learning_rate': 3.5087719298245615e-06, 'epoch': 0.9312813904792585}
2024-12-25 01:14:25,308 - INFO - Training Logs at step 8050: {'loss': 22.3855, 'grad_norm': 14.377283096313477, 'learning_rate': 3.2124229492650545e-06, 'epoch': 0.93713850614265}
2024-12-25 01:15:06,815 - INFO - Training Logs at step 8100: {'loss': 22.8622, 'grad_norm': 13.61146354675293, 'learning_rate': 2.916073968705548e-06, 'epoch': 0.9429956218060416}
2024-12-25 01:15:48,106 - INFO - Training Logs at step 8150: {'loss': 23.1297, 'grad_norm': 13.977191925048828, 'learning_rate': 2.625651967757231e-06, 'epoch': 0.9488527374694332}
2024-12-25 01:16:29,250 - INFO - Training Logs at step 8200: {'loss': 22.9861, 'grad_norm': 16.03781509399414, 'learning_rate': 2.329302987197724e-06, 'epoch': 0.9547098531328247}
2024-12-25 01:17:10,911 - INFO - Training Logs at step 8250: {'loss': 22.9567, 'grad_norm': 15.885436058044434, 'learning_rate': 2.032954006638217e-06, 'epoch': 0.9605669687962163}
2024-12-25 01:17:52,969 - INFO - Training Logs at step 8300: {'loss': 23.4446, 'grad_norm': 15.098627090454102, 'learning_rate': 1.7366050260787102e-06, 'epoch': 0.9664240844596078}
2024-12-25 01:18:34,453 - INFO - Training Logs at step 8350: {'loss': 22.8863, 'grad_norm': 13.927603721618652, 'learning_rate': 1.4402560455192034e-06, 'epoch': 0.9722812001229995}
2024-12-25 01:19:15,854 - INFO - Training Logs at step 8400: {'loss': 23.193, 'grad_norm': 11.914910316467285, 'learning_rate': 1.1439070649596966e-06, 'epoch': 0.978138315786391}
2024-12-25 01:19:57,280 - INFO - Training Logs at step 8450: {'loss': 22.8071, 'grad_norm': 14.912424087524414, 'learning_rate': 8.475580844001896e-07, 'epoch': 0.9839954314497825}
2024-12-25 01:20:38,471 - INFO - Training Logs at step 8500: {'loss': 22.4326, 'grad_norm': 11.29202651977539, 'learning_rate': 5.512091038406828e-07, 'epoch': 0.9898525471131742}
2024-12-25 01:21:09,602 - INFO - Model ytu-ce-cosmos/turkish-gpt2-medium ince ayar işlemi başarıyla tamamlandı.
2024-12-25 01:21:09,603 - INFO - Model ve tokenizer kaydediliyor: ./models/turkish-gpt2-medium_v3
2024-12-25 01:21:14,414 - INFO - Model ./models/turkish-gpt2-medium_v3 dizinine başarıyla kaydedildi.
2024-12-25 01:21:14,414 - INFO - [SUCCESS] Eğitim tamamlandı: Model=ytu-ce-cosmos/turkish-gpt2-medium, Dataset=v3
