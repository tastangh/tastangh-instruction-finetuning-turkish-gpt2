2024-12-25 00:05:47,666 - INFO - [INFO] Eğitim başlıyor: Model=ytu-ce-cosmos/turkish-gpt2-large, Dataset=v2
2024-12-25 00:05:51,922 - INFO - Dataset başarıyla yüklendi: v2
2024-12-25 00:05:51,923 - INFO - Model ve tokenizer yükleniyor: ytu-ce-cosmos/turkish-gpt2-large
2024-12-25 00:07:11,575 - INFO - Model ve tokenizer başarıyla yüklendi: ytu-ce-cosmos/turkish-gpt2-large
2024-12-25 00:07:11,575 - INFO - Model ytu-ce-cosmos/turkish-gpt2-large ince ayar işlemi başlatılıyor.
2024-12-25 00:07:29,441 - INFO - Dataset başarıyla işlendi: max_seq_length=256
2024-12-25 00:09:45,048 - INFO - Training Logs at step 100: {'loss': 27.2972, 'grad_norm': 4.913666725158691, 'learning_rate': 2.5e-05, 'epoch': 0.007757500533328162}
2024-12-25 00:10:51,768 - INFO - Training Logs at step 150: {'loss': 25.7944, 'grad_norm': 11.581676483154297, 'learning_rate': 5e-05, 'epoch': 0.015515001066656323}
2024-12-25 00:11:58,540 - INFO - Training Logs at step 200: {'loss': 24.1879, 'grad_norm': 9.278474807739258, 'learning_rate': 4.9605988967691096e-05, 'epoch': 0.023272501599984487}
2024-12-25 00:13:06,364 - INFO - Training Logs at step 250: {'loss': 23.4972, 'grad_norm': 9.371825218200684, 'learning_rate': 4.921197793538219e-05, 'epoch': 0.031030002133312647}
2024-12-25 00:14:13,329 - INFO - Training Logs at step 300: {'loss': 23.1954, 'grad_norm': 12.012864112854004, 'learning_rate': 4.8817966903073283e-05, 'epoch': 0.03878750266664081}
2024-12-25 00:15:20,116 - INFO - Training Logs at step 350: {'loss': 23.022, 'grad_norm': 10.85945987701416, 'learning_rate': 4.8423955870764384e-05, 'epoch': 0.04654500319996897}
2024-12-25 00:16:26,970 - INFO - Training Logs at step 400: {'loss': 22.5612, 'grad_norm': 9.718527793884277, 'learning_rate': 4.8029944838455484e-05, 'epoch': 0.05430250373329713}
2024-12-25 00:17:33,464 - INFO - Training Logs at step 450: {'loss': 22.9261, 'grad_norm': 10.983484268188477, 'learning_rate': 4.763593380614658e-05, 'epoch': 0.06206000426662529}
2024-12-25 00:18:39,339 - INFO - Training Logs at step 500: {'loss': 22.7949, 'grad_norm': 16.01471710205078, 'learning_rate': 4.724192277383767e-05, 'epoch': 0.06981750479995345}
2024-12-25 00:19:45,866 - INFO - Training Logs at step 550: {'loss': 22.5994, 'grad_norm': 12.757468223571777, 'learning_rate': 4.6847911741528766e-05, 'epoch': 0.07757500533328161}
2024-12-25 00:20:51,480 - INFO - Training Logs at step 600: {'loss': 22.7091, 'grad_norm': 17.44135284423828, 'learning_rate': 4.645390070921986e-05, 'epoch': 0.08533250586660977}
2024-12-25 00:21:57,092 - INFO - Training Logs at step 650: {'loss': 22.6144, 'grad_norm': 14.861686706542969, 'learning_rate': 4.605988967691095e-05, 'epoch': 0.09309000639993795}
2024-12-25 00:23:02,816 - INFO - Training Logs at step 700: {'loss': 22.4014, 'grad_norm': 14.094860076904297, 'learning_rate': 4.5665878644602053e-05, 'epoch': 0.1008475069332661}
2024-12-25 00:24:08,642 - INFO - Training Logs at step 750: {'loss': 22.4807, 'grad_norm': 12.424301147460938, 'learning_rate': 4.527186761229315e-05, 'epoch': 0.10860500746659427}
2024-12-25 00:25:14,229 - INFO - Training Logs at step 800: {'loss': 22.1481, 'grad_norm': 14.926090240478516, 'learning_rate': 4.487785657998424e-05, 'epoch': 0.11636250799992243}
2024-12-25 00:26:19,782 - INFO - Training Logs at step 850: {'loss': 22.7055, 'grad_norm': 8.934473991394043, 'learning_rate': 4.4483845547675334e-05, 'epoch': 0.12412000853325059}
2024-12-25 00:27:25,314 - INFO - Training Logs at step 900: {'loss': 22.02, 'grad_norm': 13.18704605102539, 'learning_rate': 4.4089834515366435e-05, 'epoch': 0.13187750906657875}
2024-12-25 00:28:30,809 - INFO - Training Logs at step 950: {'loss': 22.7049, 'grad_norm': 14.289043426513672, 'learning_rate': 4.369582348305753e-05, 'epoch': 0.1396350095999069}
2024-12-25 00:29:36,244 - INFO - Training Logs at step 1000: {'loss': 22.5762, 'grad_norm': 11.090003967285156, 'learning_rate': 4.330181245074862e-05, 'epoch': 0.14739251013323507}
2024-12-25 00:30:41,895 - INFO - Training Logs at step 1050: {'loss': 22.4615, 'grad_norm': 14.622002601623535, 'learning_rate': 4.2907801418439716e-05, 'epoch': 0.15515001066656323}
2024-12-25 00:31:47,483 - INFO - Training Logs at step 1100: {'loss': 22.76, 'grad_norm': 9.621115684509277, 'learning_rate': 4.2513790386130817e-05, 'epoch': 0.16290751119989139}
2024-12-25 00:32:52,957 - INFO - Training Logs at step 1150: {'loss': 22.4222, 'grad_norm': 10.4855375289917, 'learning_rate': 4.2127659574468086e-05, 'epoch': 0.17066501173321955}
2024-12-25 00:33:58,465 - INFO - Training Logs at step 1200: {'loss': 22.6558, 'grad_norm': 12.247749328613281, 'learning_rate': 4.173364854215918e-05, 'epoch': 0.1784225122665477}
2024-12-25 00:35:04,001 - INFO - Training Logs at step 1250: {'loss': 21.9815, 'grad_norm': 11.183688163757324, 'learning_rate': 4.1339637509850274e-05, 'epoch': 0.1861800127998759}
2024-12-25 00:36:09,565 - INFO - Training Logs at step 1300: {'loss': 21.9955, 'grad_norm': 12.120357513427734, 'learning_rate': 4.0945626477541374e-05, 'epoch': 0.19393751333320405}
2024-12-25 00:37:15,209 - INFO - Training Logs at step 1350: {'loss': 22.5508, 'grad_norm': 12.03931999206543, 'learning_rate': 4.055161544523247e-05, 'epoch': 0.2016950138665322}
2024-12-25 00:38:20,812 - INFO - Training Logs at step 1400: {'loss': 22.1128, 'grad_norm': 9.802202224731445, 'learning_rate': 4.015760441292357e-05, 'epoch': 0.20945251439986037}
2024-12-25 00:39:26,494 - INFO - Training Logs at step 1450: {'loss': 22.0188, 'grad_norm': 17.068584442138672, 'learning_rate': 3.976359338061466e-05, 'epoch': 0.21721001493318853}
2024-12-25 00:40:32,064 - INFO - Training Logs at step 1500: {'loss': 22.7003, 'grad_norm': 11.39387321472168, 'learning_rate': 3.9369582348305756e-05, 'epoch': 0.2249675154665167}
2024-12-25 00:41:37,631 - INFO - Training Logs at step 1550: {'loss': 22.3513, 'grad_norm': 10.046309471130371, 'learning_rate': 3.897557131599685e-05, 'epoch': 0.23272501599984485}
2024-12-25 00:42:42,857 - INFO - Training Logs at step 1600: {'loss': 22.2661, 'grad_norm': 8.947783470153809, 'learning_rate': 3.858156028368794e-05, 'epoch': 0.240482516533173}
2024-12-25 00:43:48,496 - INFO - Training Logs at step 1650: {'loss': 22.1687, 'grad_norm': 14.265159606933594, 'learning_rate': 3.818754925137904e-05, 'epoch': 0.24824001706650117}
2024-12-25 00:44:53,940 - INFO - Training Logs at step 1700: {'loss': 21.9594, 'grad_norm': 13.598847389221191, 'learning_rate': 3.779353821907014e-05, 'epoch': 0.25599751759982936}
2024-12-25 00:45:59,515 - INFO - Training Logs at step 1750: {'loss': 22.0734, 'grad_norm': 13.077171325683594, 'learning_rate': 3.739952718676123e-05, 'epoch': 0.2637550181331575}
2024-12-25 00:47:05,117 - INFO - Training Logs at step 1800: {'loss': 21.8759, 'grad_norm': 14.47286319732666, 'learning_rate': 3.7005516154452325e-05, 'epoch': 0.2715125186664857}
2024-12-25 00:48:10,743 - INFO - Training Logs at step 1850: {'loss': 22.1276, 'grad_norm': 15.583470344543457, 'learning_rate': 3.661150512214342e-05, 'epoch': 0.2792700191998138}
2024-12-25 00:49:16,361 - INFO - Training Logs at step 1900: {'loss': 21.9429, 'grad_norm': 10.250947952270508, 'learning_rate': 3.6225374310480695e-05, 'epoch': 0.287027519733142}
2024-12-25 00:50:21,952 - INFO - Training Logs at step 1950: {'loss': 22.0598, 'grad_norm': 13.738333702087402, 'learning_rate': 3.583136327817179e-05, 'epoch': 0.29478502026647013}
2024-12-25 00:51:27,718 - INFO - Training Logs at step 2000: {'loss': 22.3975, 'grad_norm': 12.222500801086426, 'learning_rate': 3.543735224586289e-05, 'epoch': 0.3025425207997983}
2024-12-25 00:52:33,359 - INFO - Training Logs at step 2050: {'loss': 22.2683, 'grad_norm': 12.714521408081055, 'learning_rate': 3.504334121355398e-05, 'epoch': 0.31030002133312645}
2024-12-25 00:53:38,576 - INFO - Training Logs at step 2100: {'loss': 22.1089, 'grad_norm': 16.260448455810547, 'learning_rate': 3.464933018124508e-05, 'epoch': 0.31805752186645464}
2024-12-25 00:54:43,954 - INFO - Training Logs at step 2150: {'loss': 22.0341, 'grad_norm': 12.3737154006958, 'learning_rate': 3.425531914893617e-05, 'epoch': 0.32581502239978277}
2024-12-25 00:55:49,212 - INFO - Training Logs at step 2200: {'loss': 21.6529, 'grad_norm': 13.373551368713379, 'learning_rate': 3.3861308116627264e-05, 'epoch': 0.33357252293311096}
2024-12-25 00:56:54,297 - INFO - Training Logs at step 2250: {'loss': 21.8377, 'grad_norm': 15.721314430236816, 'learning_rate': 3.346729708431836e-05, 'epoch': 0.3413300234664391}
2024-12-25 00:57:59,231 - INFO - Training Logs at step 2300: {'loss': 21.9438, 'grad_norm': 16.69798469543457, 'learning_rate': 3.307328605200946e-05, 'epoch': 0.3490875239997673}
2024-12-25 00:59:04,346 - INFO - Training Logs at step 2350: {'loss': 21.6331, 'grad_norm': 13.329185485839844, 'learning_rate': 3.267927501970056e-05, 'epoch': 0.3568450245330954}
2024-12-25 01:00:09,571 - INFO - Training Logs at step 2400: {'loss': 22.0095, 'grad_norm': 13.532944679260254, 'learning_rate': 3.228526398739165e-05, 'epoch': 0.3646025250664236}
2024-12-25 01:01:14,666 - INFO - Training Logs at step 2450: {'loss': 21.9969, 'grad_norm': 12.644291877746582, 'learning_rate': 3.1891252955082746e-05, 'epoch': 0.3723600255997518}
2024-12-25 01:02:19,671 - INFO - Training Logs at step 2500: {'loss': 21.6111, 'grad_norm': 11.64319133758545, 'learning_rate': 3.149724192277384e-05, 'epoch': 0.3801175261330799}
2024-12-25 01:03:24,645 - INFO - Training Logs at step 2550: {'loss': 21.6999, 'grad_norm': 10.17357063293457, 'learning_rate': 3.1103230890464934e-05, 'epoch': 0.3878750266664081}
2024-12-25 01:04:29,513 - INFO - Training Logs at step 2600: {'loss': 21.8067, 'grad_norm': 11.990504264831543, 'learning_rate': 3.070921985815603e-05, 'epoch': 0.39563252719973624}
2024-12-25 01:05:34,712 - INFO - Training Logs at step 2650: {'loss': 21.8934, 'grad_norm': 13.852435111999512, 'learning_rate': 3.0315208825847124e-05, 'epoch': 0.4033900277330644}
2024-12-25 01:06:39,749 - INFO - Training Logs at step 2700: {'loss': 21.9243, 'grad_norm': 12.133049964904785, 'learning_rate': 2.9921197793538218e-05, 'epoch': 0.41114752826639256}
2024-12-25 01:07:44,655 - INFO - Training Logs at step 2750: {'loss': 22.1765, 'grad_norm': 14.852370262145996, 'learning_rate': 2.9527186761229315e-05, 'epoch': 0.41890502879972075}
2024-12-25 01:08:49,530 - INFO - Training Logs at step 2800: {'loss': 21.9517, 'grad_norm': 13.135528564453125, 'learning_rate': 2.913317572892041e-05, 'epoch': 0.4266625293330489}
2024-12-25 01:09:54,411 - INFO - Training Logs at step 2850: {'loss': 22.1792, 'grad_norm': 12.874847412109375, 'learning_rate': 2.873916469661151e-05, 'epoch': 0.43442002986637707}
2024-12-25 01:11:00,460 - INFO - Training Logs at step 2900: {'loss': 22.1048, 'grad_norm': 14.117229461669922, 'learning_rate': 2.8345153664302603e-05, 'epoch': 0.4421775303997052}
2024-12-25 01:12:07,434 - INFO - Training Logs at step 2950: {'loss': 21.4881, 'grad_norm': 13.799090385437012, 'learning_rate': 2.79511426319937e-05, 'epoch': 0.4499350309330334}
2024-12-25 01:13:13,633 - INFO - Training Logs at step 3000: {'loss': 22.1298, 'grad_norm': 12.713452339172363, 'learning_rate': 2.7557131599684794e-05, 'epoch': 0.4576925314663615}
2024-12-25 01:14:20,071 - INFO - Training Logs at step 3050: {'loss': 22.0649, 'grad_norm': 12.153233528137207, 'learning_rate': 2.7163120567375888e-05, 'epoch': 0.4654500319996897}
2024-12-25 01:15:25,396 - INFO - Training Logs at step 3100: {'loss': 21.7136, 'grad_norm': 13.18209171295166, 'learning_rate': 2.676910953506698e-05, 'epoch': 0.47320753253301784}
2024-12-25 01:16:30,558 - INFO - Training Logs at step 3150: {'loss': 21.1024, 'grad_norm': 11.919600486755371, 'learning_rate': 2.637509850275808e-05, 'epoch': 0.480965033066346}
2024-12-25 01:17:35,727 - INFO - Training Logs at step 3200: {'loss': 21.5561, 'grad_norm': 22.397916793823242, 'learning_rate': 2.5981087470449172e-05, 'epoch': 0.48872253359967416}
2024-12-25 01:18:41,035 - INFO - Training Logs at step 3250: {'loss': 21.7794, 'grad_norm': 11.732714653015137, 'learning_rate': 2.5587076438140266e-05, 'epoch': 0.49648003413300235}
2024-12-25 01:19:46,303 - INFO - Training Logs at step 3300: {'loss': 21.6118, 'grad_norm': 14.252924919128418, 'learning_rate': 2.5193065405831363e-05, 'epoch': 0.5042375346663305}
2024-12-25 01:20:51,659 - INFO - Training Logs at step 3350: {'loss': 22.1611, 'grad_norm': 17.54859161376953, 'learning_rate': 2.479905437352246e-05, 'epoch': 0.5119950351996587}
2024-12-25 01:21:57,018 - INFO - Training Logs at step 3400: {'loss': 21.6216, 'grad_norm': 12.53652286529541, 'learning_rate': 2.4405043341213554e-05, 'epoch': 0.5197525357329869}
2024-12-25 01:23:02,221 - INFO - Training Logs at step 3450: {'loss': 22.1854, 'grad_norm': 13.230549812316895, 'learning_rate': 2.401103230890465e-05, 'epoch': 0.527510036266315}
2024-12-25 01:24:07,377 - INFO - Training Logs at step 3500: {'loss': 21.7485, 'grad_norm': 9.121260643005371, 'learning_rate': 2.3617021276595748e-05, 'epoch': 0.5352675367996431}
2024-12-25 01:25:12,650 - INFO - Training Logs at step 3550: {'loss': 21.3241, 'grad_norm': 13.282001495361328, 'learning_rate': 2.322301024428684e-05, 'epoch': 0.5430250373329714}
2024-12-25 01:26:18,142 - INFO - Training Logs at step 3600: {'loss': 21.3929, 'grad_norm': 14.182469367980957, 'learning_rate': 2.2828999211977935e-05, 'epoch': 0.5507825378662995}
2024-12-25 01:27:23,235 - INFO - Training Logs at step 3650: {'loss': 21.3603, 'grad_norm': 18.093664169311523, 'learning_rate': 2.2434988179669032e-05, 'epoch': 0.5585400383996276}
2024-12-25 01:28:28,439 - INFO - Training Logs at step 3700: {'loss': 21.7367, 'grad_norm': 14.767332077026367, 'learning_rate': 2.204097714736013e-05, 'epoch': 0.5662975389329558}
2024-12-25 01:29:33,508 - INFO - Training Logs at step 3750: {'loss': 21.5797, 'grad_norm': 12.380219459533691, 'learning_rate': 2.1646966115051223e-05, 'epoch': 0.574055039466284}
2024-12-25 01:30:38,617 - INFO - Training Logs at step 3800: {'loss': 21.6174, 'grad_norm': 12.846500396728516, 'learning_rate': 2.1252955082742317e-05, 'epoch': 0.5818125399996121}
2024-12-25 01:31:43,931 - INFO - Training Logs at step 3850: {'loss': 21.6368, 'grad_norm': 10.962818145751953, 'learning_rate': 2.0858944050433414e-05, 'epoch': 0.5895700405329403}
2024-12-25 01:32:49,014 - INFO - Training Logs at step 3900: {'loss': 21.8772, 'grad_norm': 14.190387725830078, 'learning_rate': 2.0464933018124508e-05, 'epoch': 0.5973275410662684}
2024-12-25 01:33:54,078 - INFO - Training Logs at step 3950: {'loss': 21.5969, 'grad_norm': 17.17119598388672, 'learning_rate': 2.0070921985815605e-05, 'epoch': 0.6050850415995966}
2024-12-25 01:34:59,081 - INFO - Training Logs at step 4000: {'loss': 22.1348, 'grad_norm': 13.779013633728027, 'learning_rate': 1.96769109535067e-05, 'epoch': 0.6128425421329248}
2024-12-25 01:36:04,173 - INFO - Training Logs at step 4050: {'loss': 21.602, 'grad_norm': 13.210036277770996, 'learning_rate': 1.9282899921197795e-05, 'epoch': 0.6206000426662529}
2024-12-25 01:37:09,438 - INFO - Training Logs at step 4100: {'loss': 21.5728, 'grad_norm': 19.1545352935791, 'learning_rate': 1.888888888888889e-05, 'epoch': 0.6283575431995811}
2024-12-25 01:38:14,496 - INFO - Training Logs at step 4150: {'loss': 21.8161, 'grad_norm': 13.762165069580078, 'learning_rate': 1.8494877856579983e-05, 'epoch': 0.6361150437329093}
2024-12-25 01:39:19,643 - INFO - Training Logs at step 4200: {'loss': 21.6191, 'grad_norm': 12.019851684570312, 'learning_rate': 1.8100866824271083e-05, 'epoch': 0.6438725442662374}
2024-12-25 01:40:25,007 - INFO - Training Logs at step 4250: {'loss': 21.399, 'grad_norm': 13.684577941894531, 'learning_rate': 1.7706855791962177e-05, 'epoch': 0.6516300447995655}
2024-12-25 01:41:30,251 - INFO - Training Logs at step 4300: {'loss': 21.6933, 'grad_norm': 7.233274936676025, 'learning_rate': 1.731284475965327e-05, 'epoch': 0.6593875453328938}
2024-12-25 01:42:35,379 - INFO - Training Logs at step 4350: {'loss': 21.7635, 'grad_norm': 15.174026489257812, 'learning_rate': 1.6918833727344364e-05, 'epoch': 0.6671450458662219}
2024-12-25 01:43:40,520 - INFO - Training Logs at step 4400: {'loss': 21.3864, 'grad_norm': 11.48518180847168, 'learning_rate': 1.652482269503546e-05, 'epoch': 0.67490254639955}
2024-12-25 01:44:45,633 - INFO - Training Logs at step 4450: {'loss': 21.4444, 'grad_norm': 13.97914981842041, 'learning_rate': 1.613081166272656e-05, 'epoch': 0.6826600469328782}
2024-12-25 01:45:50,567 - INFO - Training Logs at step 4500: {'loss': 21.753, 'grad_norm': 7.861978054046631, 'learning_rate': 1.5736800630417652e-05, 'epoch': 0.6904175474662064}
2024-12-25 01:46:56,005 - INFO - Training Logs at step 4550: {'loss': 21.4372, 'grad_norm': 13.266898155212402, 'learning_rate': 1.534278959810875e-05, 'epoch': 0.6981750479995346}
2024-12-25 01:48:01,065 - INFO - Training Logs at step 4600: {'loss': 21.8977, 'grad_norm': 13.231118202209473, 'learning_rate': 1.4948778565799843e-05, 'epoch': 0.7059325485328627}
2024-12-25 01:49:06,193 - INFO - Training Logs at step 4650: {'loss': 21.9662, 'grad_norm': 10.385496139526367, 'learning_rate': 1.4554767533490937e-05, 'epoch': 0.7136900490661908}
2024-12-25 01:50:11,284 - INFO - Training Logs at step 4700: {'loss': 21.9401, 'grad_norm': 12.194061279296875, 'learning_rate': 1.4160756501182036e-05, 'epoch': 0.7214475495995191}
2024-12-25 01:51:16,346 - INFO - Training Logs at step 4750: {'loss': 21.5938, 'grad_norm': 14.416890144348145, 'learning_rate': 1.376674546887313e-05, 'epoch': 0.7292050501328472}
2024-12-25 01:52:21,318 - INFO - Training Logs at step 4800: {'loss': 21.4214, 'grad_norm': 16.148778915405273, 'learning_rate': 1.3372734436564225e-05, 'epoch': 0.7369625506661753}
2024-12-25 01:53:26,328 - INFO - Training Logs at step 4850: {'loss': 21.5282, 'grad_norm': 13.262502670288086, 'learning_rate': 1.2978723404255318e-05, 'epoch': 0.7447200511995036}
2024-12-25 01:54:31,231 - INFO - Training Logs at step 4900: {'loss': 21.676, 'grad_norm': 10.095369338989258, 'learning_rate': 1.2584712371946414e-05, 'epoch': 0.7524775517328317}
2024-12-25 01:55:36,216 - INFO - Training Logs at step 4950: {'loss': 21.7833, 'grad_norm': 13.402005195617676, 'learning_rate': 1.219070133963751e-05, 'epoch': 0.7602350522661598}
2024-12-25 01:56:41,289 - INFO - Training Logs at step 5000: {'loss': 21.8521, 'grad_norm': 11.608766555786133, 'learning_rate': 1.1796690307328606e-05, 'epoch': 0.767992552799488}
2024-12-25 01:57:46,415 - INFO - Training Logs at step 5050: {'loss': 21.2831, 'grad_norm': 14.779511451721191, 'learning_rate': 1.1402679275019702e-05, 'epoch': 0.7757500533328162}
2024-12-25 01:58:51,392 - INFO - Training Logs at step 5100: {'loss': 21.3544, 'grad_norm': 10.126741409301758, 'learning_rate': 1.1008668242710797e-05, 'epoch': 0.7835075538661443}
2024-12-25 01:59:56,232 - INFO - Training Logs at step 5150: {'loss': 21.551, 'grad_norm': 10.709601402282715, 'learning_rate': 1.0614657210401892e-05, 'epoch': 0.7912650543994725}
2024-12-25 02:01:02,144 - INFO - Training Logs at step 5200: {'loss': 21.8507, 'grad_norm': 13.474645614624023, 'learning_rate': 1.0220646178092986e-05, 'epoch': 0.7990225549328006}
2024-12-25 02:02:07,491 - INFO - Training Logs at step 5250: {'loss': 21.3789, 'grad_norm': 11.178317070007324, 'learning_rate': 9.826635145784083e-06, 'epoch': 0.8067800554661289}
2024-12-25 02:03:12,595 - INFO - Training Logs at step 5300: {'loss': 21.3914, 'grad_norm': 13.018091201782227, 'learning_rate': 9.432624113475177e-06, 'epoch': 0.814537555999457}
2024-12-25 02:04:17,749 - INFO - Training Logs at step 5350: {'loss': 21.2006, 'grad_norm': 16.36846351623535, 'learning_rate': 9.038613081166274e-06, 'epoch': 0.8222950565327851}
2024-12-25 02:05:22,906 - INFO - Training Logs at step 5400: {'loss': 21.5555, 'grad_norm': 11.034382820129395, 'learning_rate': 8.644602048857368e-06, 'epoch': 0.8300525570661133}
2024-12-25 02:06:27,997 - INFO - Training Logs at step 5450: {'loss': 21.0607, 'grad_norm': 15.95295524597168, 'learning_rate': 8.250591016548463e-06, 'epoch': 0.8378100575994415}
2024-12-25 02:07:33,410 - INFO - Training Logs at step 5500: {'loss': 21.619, 'grad_norm': 14.676202774047852, 'learning_rate': 7.85657998423956e-06, 'epoch': 0.8455675581327696}
2024-12-25 02:08:39,679 - INFO - Training Logs at step 5550: {'loss': 21.5761, 'grad_norm': 13.982051849365234, 'learning_rate': 7.462568951930654e-06, 'epoch': 0.8533250586660978}
2024-12-25 02:09:45,572 - INFO - Training Logs at step 5600: {'loss': 21.4167, 'grad_norm': 14.054645538330078, 'learning_rate': 7.06855791962175e-06, 'epoch': 0.8610825591994259}
2024-12-25 02:10:50,486 - INFO - Training Logs at step 5650: {'loss': 21.3274, 'grad_norm': 13.289083480834961, 'learning_rate': 6.674546887312845e-06, 'epoch': 0.8688400597327541}
2024-12-25 02:11:55,700 - INFO - Training Logs at step 5700: {'loss': 21.5405, 'grad_norm': 9.619508743286133, 'learning_rate': 6.28053585500394e-06, 'epoch': 0.8765975602660823}
2024-12-25 02:13:01,399 - INFO - Training Logs at step 5750: {'loss': 21.4806, 'grad_norm': 14.611414909362793, 'learning_rate': 5.886524822695036e-06, 'epoch': 0.8843550607994104}
2024-12-25 02:14:06,587 - INFO - Training Logs at step 5800: {'loss': 21.9331, 'grad_norm': 13.301690101623535, 'learning_rate': 5.492513790386132e-06, 'epoch': 0.8921125613327386}
2024-12-25 02:15:11,970 - INFO - Training Logs at step 5850: {'loss': 21.7073, 'grad_norm': 12.082574844360352, 'learning_rate': 5.098502758077226e-06, 'epoch': 0.8998700618660668}
2024-12-25 02:16:17,071 - INFO - Training Logs at step 5900: {'loss': 21.6076, 'grad_norm': 15.98468017578125, 'learning_rate': 4.7044917257683215e-06, 'epoch': 0.9076275623993949}
2024-12-25 02:17:22,895 - INFO - Training Logs at step 5950: {'loss': 21.6313, 'grad_norm': 12.678654670715332, 'learning_rate': 4.310480693459417e-06, 'epoch': 0.915385062932723}
2024-12-25 02:18:28,710 - INFO - Training Logs at step 6000: {'loss': 21.6195, 'grad_norm': 10.964079856872559, 'learning_rate': 3.916469661150512e-06, 'epoch': 0.9231425634660513}
2024-12-25 02:19:34,591 - INFO - Training Logs at step 6050: {'loss': 21.3614, 'grad_norm': 13.081740379333496, 'learning_rate': 3.522458628841608e-06, 'epoch': 0.9309000639993794}
2024-12-25 02:20:39,684 - INFO - Training Logs at step 6100: {'loss': 21.5302, 'grad_norm': 14.58072566986084, 'learning_rate': 3.1284475965327027e-06, 'epoch': 0.9386575645327075}
2024-12-25 02:21:44,901 - INFO - Training Logs at step 6150: {'loss': 21.306, 'grad_norm': 12.077661514282227, 'learning_rate': 2.7344365642237985e-06, 'epoch': 0.9464150650660357}
2024-12-25 02:22:50,214 - INFO - Training Logs at step 6200: {'loss': 21.3231, 'grad_norm': 13.422883987426758, 'learning_rate': 2.3404255319148935e-06, 'epoch': 0.9541725655993639}
2024-12-25 02:23:55,573 - INFO - Training Logs at step 6250: {'loss': 21.7117, 'grad_norm': 16.513460159301758, 'learning_rate': 1.9464144996059893e-06, 'epoch': 0.961930066132692}
2024-12-25 02:25:00,913 - INFO - Training Logs at step 6300: {'loss': 21.3733, 'grad_norm': 12.556480407714844, 'learning_rate': 1.5524034672970845e-06, 'epoch': 0.9696875666660202}
2024-12-25 02:26:06,243 - INFO - Training Logs at step 6350: {'loss': 21.1261, 'grad_norm': 14.398629188537598, 'learning_rate': 1.1583924349881796e-06, 'epoch': 0.9774450671993483}
2024-12-25 02:27:11,577 - INFO - Training Logs at step 6400: {'loss': 21.3765, 'grad_norm': 16.62236785888672, 'learning_rate': 7.64381402679275e-07, 'epoch': 0.9852025677326766}
2024-12-25 02:28:11,670 - INFO - Model ytu-ce-cosmos/turkish-gpt2-large ince ayar işlemi başarıyla tamamlandı.
2024-12-25 02:28:11,672 - INFO - Model ve tokenizer kaydediliyor: ./models/turkish-gpt2-large_v2
2024-12-25 02:28:19,090 - INFO - Model ./models/turkish-gpt2-large_v2 dizinine başarıyla kaydedildi.
2024-12-25 02:28:19,091 - INFO - [SUCCESS] Eğitim tamamlandı: Model=ytu-ce-cosmos/turkish-gpt2-large, Dataset=v2
